<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> In Un'Altra Vita</title><meta name="description" content="A Blog Powered By Hexo"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/mugen.ico"><link rel="stylesheet" href="/css/apollo.css"><!-- link(rel="stylesheet", href=url_for("https://highlightjs.org/static/demo/styles/" + (theme.codestyle ? theme.codestyle : 'solarized-light') + ".css"))--><link rel="search" type="application/opensearchdescription+xml" href="https://lamply.github.io/atom.xml" title="In Un'Altra Vita"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/mugen.ico" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link active">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/Lamply" target="_blank" class="nav-list-link">GITHUB</a></li></ul></header><main class="container"><ul class="home post-list"><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2019/10/16/Algorithm_Acceleration/" class="post-title-link">算法加速的几种方法</a></h2><div class="tags"><a href="/tags/技术经验/" class="tag-title">#技术经验</a></div><div class="post-info">Oct 16, 2019</div><div class="post-content"><p>这里整理一下常见的算法加速方法。一般来说，对于高计算量的算法，加速是必然要考虑的事情。理想情况下，在编写算法时就应该考虑让算法便于优化，下面就针对性谈谈。<br></p></div><a href="/2019/10/16/Algorithm_Acceleration/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2019/04/19/protrait-segmentation/" class="post-title-link">人像分割</a></h2><div class="tags"><a href="/tags/技术经验/" class="tag-title">#技术经验</a><a href="/tags/语义分割/" class="tag-title">#语义分割</a></div><div class="post-info">Apr 19, 2019</div><div class="post-content"><div align="center">
<img src="/2019/04/19/protrait-segmentation/Einstein.png" width="70%">  
</div>

<p>这部分是关于在低计算量下完成人像分割的工作，因为时间充裕，所以调查尝试得比较多，最终完成的效果还不错。<br></p></div><a href="/2019/04/19/protrait-segmentation/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/08/07/Low_Rank_Research/" class="post-title-link">关于 LRA 和 Force Regularization 的探索</a></h2><div class="tags"><a href="/tags/技术经验/" class="tag-title">#技术经验</a><a href="/tags/模型加速和压缩/" class="tag-title">#模型加速和压缩</a></div><div class="post-info">Aug 7, 2018</div><div class="post-content"><p>这部分是将《Coordinating Filters for Faster Deep Neural Networks》中提到的 <em>Force Regularization</em> 和 <em>LRA</em> 用于实际项目的效果，虽然现在看来不是很严谨，不过算是一次很好的尝试。<br></p></div><a href="/2018/08/07/Low_Rank_Research/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/08/07/caffe_things/" class="post-title-link">Caffe使用问题记录</a></h2><div class="tags"><a href="/tags/技术经验/" class="tag-title">#技术经验</a><a href="/tags/问题记录/" class="tag-title">#问题记录</a><a href="/tags/caffe/" class="tag-title">#caffe</a></div><div class="post-info">Aug 7, 2018</div><div class="post-content"><p>以往在使用 caffe 中遇到的部分问题记录。<br></p></div><a href="/2018/08/07/caffe_things/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/07/19/face-alignment-cnn/" class="post-title-link">深度学习方法的人脸对齐</a></h2><div class="tags"><a href="/tags/技术经验/" class="tag-title">#技术经验</a><a href="/tags/人脸对齐/" class="tag-title">#人脸对齐</a></div><div class="post-info">Jul 19, 2018</div><div class="post-content"><figure align="center">
<img src="/2018/07/19/face-alignment-cnn/1.png">
</figure>

<p>这部分是去年 9 月份开始的工作，算是第一次真正踏入深度学习的领域。具体工作也还算简单，就是复现一篇深度学习方法做的人脸对齐，当练练手。<br></p></div><a href="/2018/07/19/face-alignment-cnn/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/07/15/face-alignment/" class="post-title-link">传统方法的人脸对齐</a></h2><div class="tags"><a href="/tags/技术经验/" class="tag-title">#技术经验</a><a href="/tags/人脸对齐/" class="tag-title">#人脸对齐</a></div><div class="post-info">Jul 15, 2018</div><div class="post-content"><p><div align="center">
<img src="/2018/07/15/face-alignment/1.png">  
</div><br>这里是关于应用传统方法做人脸对齐的经验总结，是在去年5月到7月的工作，也是我入职后的第一个正式项目，用的是 SDM (Supervised Descent Method) [1] 的方法，具体细节可能不太记得，所以会慢慢补完。<br></p></div><a href="/2018/07/15/face-alignment/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/04/19/zero-kernels/" class="post-title-link">零核现象</a></h2><div class="tags"><a href="/tags/观测/" class="tag-title">#观测</a></div><div class="post-info">Apr 19, 2018</div><div class="post-content"><p><div align="center">
<img src="/2018/04/19/zero-kernels/TingMengDe.bmp">  
</div><br>这里是对零核现象的观察实验记录.<br>具体来说, 就是在训练卷积神经网络的过程中发现模型中有大量卷积核的 L1 变为 0 的情况, 这里为了方便简称零核现象.<br></p></div><a href="/2018/04/19/zero-kernels/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/03/01/MobileNetsV2/" class="post-title-link">MobileNet V2</a></h2><div class="tags"><a href="/tags/论文笔记/" class="tag-title">#论文笔记</a></div><div class="post-info">Mar 1, 2018</div><div class="post-content"><p>这是关于轻量级网络 MobileNet 的改进版论文，作为万众瞩目的高效率骨干网络架构，它的更新意味着移动端网络的又一次改进。<br>原文链接： <a href="https://arxiv.org/pdf/1801.04381.pdf" target="_blank" rel="noopener">Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation</a><br></p></div><a href="/2018/03/01/MobileNetsV2/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/03/01/DeepLab/" class="post-title-link">DeepLab系列论文简略记录</a></h2><div class="tags"><a href="/tags/论文笔记/" class="tag-title">#论文笔记</a></div><div class="post-info">Mar 1, 2018</div><div class="post-content"><p>这部分是关于语义分割网络 DeepLab 系列的三篇论文。尽管经验性的技巧很多，但就效果而言还是很不错的，有不少值得参考的地方。<br></p></div><a href="/2018/03/01/DeepLab/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/03/01/ShuffleNet/" class="post-title-link">ShuffleNet</a></h2><div class="tags"><a href="/tags/论文笔记/" class="tag-title">#论文笔记</a></div><div class="post-info">Mar 1, 2018</div><div class="post-content"><p>这部分是关于轻量级网络 ShuffleNet 的论文记录，主要是基于 channel shuffle 的想法来减少 CNN 中占大头的 1x1 卷积的计算量。<br></p></div><a href="/2018/03/01/ShuffleNet/" class="read-more">...more</a></article></li></ul></main><footer><div class="paginator"><a href="/page/2/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2020 <a href="https://lamply.github.io">Lamply</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>