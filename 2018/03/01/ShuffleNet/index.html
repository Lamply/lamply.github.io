<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> ShuffleNet · In Un'Altra Vita</title><meta name="description" content="ShuffleNet - Lamply"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/mugen.ico"><link rel="stylesheet" href="/css/apollo.css"><!-- link(rel="stylesheet", href=url_for("https://highlightjs.org/static/demo/styles/" + (theme.codestyle ? theme.codestyle : 'solarized-light') + ".css"))--><link rel="search" type="application/opensearchdescription+xml" href="https://lamply.github.io/atom.xml" title="In Un'Altra Vita"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/mugen.ico" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/Lamply" target="_blank" class="nav-list-link">GITHUB</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">ShuffleNet</h1><div class="tags"><a href="/tags/论文笔记/" class="tag-title">#论文笔记</a></div><div class="post-info">Mar 1, 2018</div><div class="post-content"><p>这部分是关于轻量级网络 ShuffleNet 的论文记录，主要是基于 channel shuffle 的想法来减少 CNN 中占大头的 1x1 卷积的计算量。<br><a id="more"></a></p>
<h3 id="ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices"><a href="#ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices" class="headerlink" title="ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"></a>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</h3><hr>
<h4 id="原文理解"><a href="#原文理解" class="headerlink" title="原文理解"></a>原文理解</h4><p>介绍一种极其计算高效的 CNN 结构 ShuffleNet, 计算量可以控制在 10-150M FLOPs, 利用了 1x1 group conv, depth-wise conv 和 shuffle channel op.<br>比起现有的方法专注于对基础网络进行 pruning, compressing, low-bit representing, 本文追求在非常有限的计算资源下 ( 几十或几百 MFLOPs ) 达到最好的效果. 目标是探索针对所需计算量范围定制非常高效的基础网络.<br>ResNext 和 Xception 由于 1x1 dense 卷积过多在小网络中效率不高, 本文提出 <em>pointwise group convolution</em> 减少 1x1 卷积的计算复杂度, 同时经过 <em>channel shuffle</em> 来克服边缘效应</p>
<h5 id="channel-shuffle"><a href="#channel-shuffle" class="headerlink" title="channel shuffle"></a>channel shuffle</h5><p>在小网络下 1x1 卷积数量大很昂贵, 减少通道可能会对精度产生很大的损害. 最直观的解决方法是采用 channel sparse connections, 比如 group conv. 但这又导致输出部分仅与其相应输入的部分通道产生, 导致信息流通阻塞.<br>通过 <em>channel shuffle</em> : 输出 g x n 通道, 对输入先 reshape 到 (g, n), 转置然后 flatten. 可以优雅地解决问题.</p>
<h5 id="ShuffleNet-Unit"><a href="#ShuffleNet-Unit" class="headerlink" title="ShuffleNet Unit"></a>ShuffleNet Unit</h5><p>基于 bottleneck 的结构 (b) (c): </p>
<p><div align="center">
<img src="/2018/03/01/ShuffleNet/Unit.png">  
</div><br>为了简单, channel shuffle 只加在第一个 1x1 后. 由于在低功耗设备下 depth-wise conv 的 <code>computation/memory  access  ratio</code> 可能比 dense op 要差, 实际上并不能高效实施, 所以故意只在 bottleneck 中使用.</p>
<h5 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h5><p>参考设计: 类似 ResNet, bottleneck channel 是 output channel 的 1/4, 下层 channel 翻一倍.<br>在 ShuffleNet 中, 大致相同计算量下, 明显可以看出, group 数越大, output channel 就要越多, 这有助于 encode 更多信息, 尽管有可能会导致单个卷积滤波器的作用降级<br>另外, 在第一次 point-wise conv 时不采用 group conv, 因为输入通道数相对较小</p>
<h5 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h5><p>和 MobileNet 一样, 这里训练使用了不那么 aggressive 的 scale augmentation, 因为小网络通常会欠拟合而不是过拟合 <strong>( 本人在 ENet 的实验中也印证了这点 )</strong>, 实验结论如下:</p>
<ol>
<li><em>pointwise group convolution</em> 有效, group 比不 group 好, Smaller models tend to benefit more from groups.</li>
<li>0.5x 情况下 group 比较大时会饱和, 准确率甚至下降. 但再减小 channel 到 0.25x, 发现增大 group 没有饱和, 反而收益更多了. <strong>( 此处实验不充分, 不足以证明什么 )</strong></li>
<li>Channel Shuffle 很有效, 特别在大 group 时.</li>
<li>实际加速由于内存访问等原因在移动平台上的呈现 理论加速 4 倍 = 实际加速 2.6 倍</li>
</ol>
</div></article></div></main><footer><div class="paginator"><a href="/2018/03/01/DeepLab/" class="prev">PREV</a><a href="/2017/11/15/Low_Rank/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'https-lamply-github-io';
var disqus_identifier = '2018/03/01/ShuffleNet/';
var disqus_title = 'ShuffleNet';
var disqus_url = 'https://lamply.github.io/2018/03/01/ShuffleNet/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//https-lamply-github-io.disqus.com/count.js" async></script><div class="copyright"><p>© 2015 - 2020 <a href="https://lamply.github.io">Lamply</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>