<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SlideWater</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://lamply.github.io/"/>
  <updated>2018-08-07T06:21:01.998Z</updated>
  <id>https://lamply.github.io/</id>
  
  <author>
    <name>Lamply</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>关于 LRA 和 Force Regularization 的探索</title>
    <link href="https://lamply.github.io/2018/08/07/Low_Rank_Research/"/>
    <id>https://lamply.github.io/2018/08/07/Low_Rank_Research/</id>
    <published>2018-08-07T05:13:06.000Z</published>
    <updated>2018-08-07T06:21:01.998Z</updated>
    
    <content type="html"><![CDATA[<p>这部分是将上一篇提到的 <em>Force Regularization</em> 和 <em>LRA</em> 用于实际项目的效果，虽然现在看来不是很严谨，不过算是一次很好的尝试。<br><a id="more"></a></p><h3 id="设定"><a href="#设定" class="headerlink" title="设定"></a>设定</h3><p>为了探索 <em>Low-Rank Approximations ( LRA )</em> 和 <em>Force Regularization</em>  ( 参考 <em>Wen. “Coordinating Filters for Faster Deep Neural Networks” ICCV 2017</em> ) 在我的工程上的实际效果, 进行了一些实际测试. 由于时间限制, 主要进行了两次探索, 分别为:</p><ul><li>LRA<ul><li>单次大降秩(rank ratio 0.48)</li><li>在大降秩的基础上小降秩(rank ratio 0.8)</li></ul></li><li>LRA + Force Regularization<ul><li>多次迭代 LRA (每次 rank ratio 0.9), FR (0.003 Degradation)</li></ul></li></ul><p>此外还有在 MNIST 上对 LeNet 的对照测试, 结果与文中叙述结论基本一致, Force Regularization 后 LRA 带来的压缩率有进一步的提高, 但主要在全连接层体现 (实验时卷积层个数完全没变), 尚未使用其他网络进行测试, 也没有观察出 Force Regularization 后卷积核的变化, 可能需要进一步实验 (调整 Force Regularization 参数, 用更好的可视化方法 t-SNE 等)</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><ul><li>原模型结果</li></ul><table><thead><tr><th style="text-align:center">测试</th><th style="text-align:center">分数</th></tr></thead><tbody><tr><td style="text-align:center">原准确率</td><td style="text-align:center">0.8993</td></tr><tr><td style="text-align:center">原召回率</td><td style="text-align:center">0.9017</td></tr><tr><td style="text-align:center">F1-Score</td><td style="text-align:center">0.8919</td></tr></tbody></table><ul><li>LRA<br>  单次 0.48 大降秩后 finetune 17300 iters, 接着 0.8 小降秩 finetune 40000 iters:</li></ul><table><thead><tr><th style="text-align:center">测试</th><th style="text-align:center">0.48分数</th><th style="text-align:center">0.48+0.8分数</th></tr></thead><tbody><tr><td style="text-align:center">准确率</td><td style="text-align:center">0.8947</td><td style="text-align:center">0.9181</td></tr><tr><td style="text-align:center">召回率</td><td style="text-align:center">0.8868</td><td style="text-align:center">0.8157</td></tr><tr><td style="text-align:center">F1-Score</td><td style="text-align:center">0.8813</td><td style="text-align:center">0.8489</td></tr></tbody></table><ul><li>LRA + Force Regularization<br>此版本由于原工程正在改进, 所以使用了新的方法 (修改了网络输出层的卷积个数以及输入的通道数, 准确率略微提高, 召回率变化不大)<br>在新方法训练的模型下进行 0.003 Degradation 的 Force Regularization, 400 iters ( 50 iters/epoch ) 后 0.9 rank ratio 降秩, finetune 1000 iters后 继续 0.9 rank ratio 降秩, finetune 1300 iters 后得到收敛结果</li></ul><table><thead><tr><th style="text-align:center">测试</th><th style="text-align:center">源模型 FR</th><th style="text-align:center">第一次降秩</th><th style="text-align:center">第二次降秩</th></tr></thead><tbody><tr><td style="text-align:center">准确率</td><td style="text-align:center">0.9252</td><td style="text-align:center">0.9207</td><td style="text-align:center">0.9228</td></tr><tr><td style="text-align:center">召回率</td><td style="text-align:center">0.7729</td><td style="text-align:center">0.8520</td><td style="text-align:center">0.8443</td></tr><tr><td style="text-align:center">F1-Score</td><td style="text-align:center">0.8298</td><td style="text-align:center">0.8731</td><td style="text-align:center">0.8698</td></tr></tbody></table><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><ol><li>从 LRA 可以看出, 单次大降秩也能恢复到接近源模型的效果(召回率下降大约2%), 模型大小压缩明显(12.7M =&gt; 4.8M), 但是再度降秩模型效果开始较大幅度下降(召回率再度下降7%), 且模型大小变化不大(4.8M =&gt; 3.7M)</li><li>FR 后模型的召回率迅速降低, 但理论上在此基础上再进行多次降秩并最终 finetune 应该是能恢复效果的, 问题 loss 已经几乎收敛, 无法看出有明显下降, 召回率仍然有较大损失, 所以怀疑可能需要降低训练 force regularization, 和 learning rate, 或者有可能是 FR 未足够 finetune 的问题??  </li></ol><h3 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h3><p>进行了FR再测试, 对原模型进行了更久的 finetune, 得到</p><table><thead><tr><th style="text-align:center">测试</th><th style="text-align:center">分数</th></tr></thead><tbody><tr><td style="text-align:center">准确率</td><td style="text-align:center">0.9194</td></tr><tr><td style="text-align:center">召回率</td><td style="text-align:center">0.9018</td></tr><tr><td style="text-align:center">F1-Score</td><td style="text-align:center">0.9030</td></tr></tbody></table><p>对于速度, 进行了几个小实验, 似乎该方法在小网络上会由于增加卷积层所以减慢前向速度, 而且比起低秩增速, 卷积层的增加带来的负面影响似乎更大. 至少以本项目来说是有些许降速的.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这部分是将上一篇提到的 &lt;em&gt;Force Regularization&lt;/em&gt; 和 &lt;em&gt;LRA&lt;/em&gt; 用于实际项目的效果，虽然现在看来不是很严谨，不过算是一次很好的尝试。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术经验" scheme="https://lamply.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BB%8F%E9%AA%8C/"/>
    
      <category term="模型加速和压缩" scheme="https://lamply.github.io/tags/%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F%E5%92%8C%E5%8E%8B%E7%BC%A9/"/>
    
  </entry>
  
  <entry>
    <title>Coordinating Filters for Faster Deep Neural Networks</title>
    <link href="https://lamply.github.io/2018/08/07/Low_Rank/"/>
    <id>https://lamply.github.io/2018/08/07/Low_Rank/</id>
    <published>2018-08-07T05:02:21.000Z</published>
    <updated>2018-08-07T06:20:00.375Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><br><img src="/2018/08/07/Low_Rank/ForceRegularization.png"><br></div><p>这篇论文是在学习压缩模型时无意中看到的，发表在 ICCV 2017。因为看到它的 motivation 觉得挺有意思的（昴星团瞩目），刚好还有代码，于是就学习了一下，顺带看看能不能用在项目上。  </p><p>原文链接： <a href="https://arxiv.org/abs/1703.09746" target="_blank" rel="noopener">Coordinating Filters for Faster Deep Neural Networks</a><br><a id="more"></a></p><h3 id="原文理解"><a href="#原文理解" class="headerlink" title="原文理解"></a>原文理解</h3><ul><li>压缩和加速 DNN 模型的工作</li><li>常规压缩 <em>sparsity-based</em> 方法 <em>Low-Rank Approximations ( LRA )</em>:<ul><li>可以不必经过仔细的硬件/软件设计就能压缩和加速 DNN</li><li>原理在于滤波器之间冗余(相关性), 把大的矩阵近似成两个小矩阵相乘</li><li>此工作专注于压缩已经训练好的模型来达到最大化减小计算复杂性, 然后 retrain 来保持精度</li></ul></li><li>本工作注重训练出 <em>Lower-Rank Space</em> 的 DNN, 提出了 <em>Force Regularization</em>:<ul><li>主要是通过引入额外梯度 ( <em>attractive forces</em> ) 微调参数来增强滤波器的相关性, 从而使得 LRA 后能获得更小的参数量</li></ul></li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>首先介绍 cross-filter LRA :<br>LRA 为将大矩阵<br>$$W\in\mathbb{R}^{N \times C \times H \times W}$$<br>分解为一个低秩矩阵和一个1x1的卷积偏移<br>$$\beta_m\in\mathbb{R}^{M \times C \times H \times W}, b\in\mathbb{R}^{1 \times C \times H \times W}$$<br>那么输出的 feature map 为:  </p><p> $$O_n\approx(\sum^M_{m=1}b_m^{(n)}\beta_m)*I = \sum^M_{m=1}(b_m^{(n)}F_m)$$  </p><p>这里的<br>$$F_m = \beta_m * I$$<br>所以输出即低秩矩阵与输入的卷积的线性组合  </p><p>从数学层面上看 <em>Force Regularization</em>:<br>$$\Delta W_i = \sum^N_{j=1}\Delta W_{ij} = ||W_i||\sum^N_{j=1}(f_{ji}-f_{ji}w_i^Tw_i)$$<br>$$W_i \gets W_i-\eta \cdot (\frac{\partial E(W)}{\partial W_i}-\lambda_s \cdot \Delta W_i)$$</p><div align="center"><br><img src="/2018/08/07/Low_Rank/fig_math.png"><br></div><p>在物理层面上看 <em>Force Regularization</em> , 像是引力将参数聚集在一起</p><blockquote><p>Suppose eachvectorwiis a rigid stick and there is a particle fixed atthe endpoint. The particle has unit mass, and the stick ismassless and can freely spin around the origin. Given thepair-wise attractive forces (e.g.,universal gravitation) f_ji,Eq. (2) is the acceleration of particlei. As the forces are at-tractive, neighbor particles tend to spin around the origin toassemble together.  </p></blockquote><p>作者认为, 增加 <em>Force Regularization</em> 可以让一簇滤波器趋向于有相同的方向, 而由于数据损失梯度的存在使得该正则项不影响原本滤波器提取有判别力的特征的能力(存疑)</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>实验使用 baseline 作为 pretrained model. 原因是在相同最大迭代次数下, 从 baseline 开始训练比从头开始要有更好的精准度和速度提升的tradeoff, 因为 pretrained model 提供了精准度和高关联性的初始化条件.  </p><p>实验结论:</p><ul><li><em>Force Regularization</em> 能在低层卷积保持低秩特性, 然后在高层卷积时有很大的压缩, 总体上看 rank ratio (低秩和全秩比) 大约为50%.  </li><li>L2norm 在高 rank ratio 时表现得比较好, L1norm 在潜在低 rank ratio 时表现得更好.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;center&quot;&gt;&lt;br&gt;&lt;img src=&quot;/2018/08/07/Low_Rank/ForceRegularization.png&quot;&gt;&lt;br&gt;&lt;/div&gt;

&lt;p&gt;这篇论文是在学习压缩模型时无意中看到的，发表在 ICCV 2017。因为看到它的 motivation 觉得挺有意思的（昴星团瞩目），刚好还有代码，于是就学习了一下，顺带看看能不能用在项目上。  &lt;/p&gt;
&lt;p&gt;原文链接： &lt;a href=&quot;https://arxiv.org/abs/1703.09746&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Coordinating Filters for Faster Deep Neural Networks&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文笔记" scheme="https://lamply.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Caffe使用问题记录</title>
    <link href="https://lamply.github.io/2018/08/07/caffe_things/"/>
    <id>https://lamply.github.io/2018/08/07/caffe_things/</id>
    <published>2018-08-07T04:44:35.000Z</published>
    <updated>2018-08-07T04:52:32.934Z</updated>
    
    <content type="html"><![CDATA[<p>以往在使用 caffe 中遇到的部分问题记录。<br><a id="more"></a></p><h4 id="缺陷记录"><a href="#缺陷记录" class="headerlink" title="缺陷记录"></a>缺陷记录</h4><ul><li><code>Xavier</code>初始化没有乘上增益 (ReLU应乘根号2, 等等)</li><li>在matlab上训练得出的模型是col-major,需要将所有矩阵参数转置才能在其他地方用</li><li>老版本caffe在初次前向时会比较慢, 新版未知</li><li>caffe 初始化数据层时启动线程是 <strong>TEST</strong> 和 <strong>TRAIN</strong> 并行进行的, 即使将<code>test_initialization</code>设置为<code>false</code>也会进行一次<strong>TEST</strong>的数据 prefetch,  同样会进行<code>Transform</code>, 所以要注意相关的共享变量.</li><li>BatchNorm 的 eps 默认为 1e-5, 这个数值是 切实 会对结果产生一定影响的, 在 absorb 参数时也要注意</li></ul><h4 id="过程记录"><a href="#过程记录" class="headerlink" title="过程记录"></a>过程记录</h4><ul><li>后向根据<code>top_diff</code>和前向结果算出各<code>blob</code>参数的<code>diff</code>, 以及<code>bottom</code>的<code>diff</code>, 所以分别对<code>blob</code>和<code>bottom</code>求导</li><li>传播时记得不同微分层乘上前面的梯度值<code>top_diff</code>,后传多个梯度值的话全部加起来</li><li><code>setup</code>是在加载网络时调用的, 加载完后不再调用</li></ul><h4 id="错误记录"><a href="#错误记录" class="headerlink" title="错误记录"></a>错误记录</h4><ol><li>Check failed: data_<ul><li>为 <code>blob shape</code> 错误, 一般是 <code>reshape</code> 函数出错, 也可能是网络设计错误导致 <code>shape</code> 传过来时负值错误</li></ul></li></ol><h4 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h4><ol><li>caffe模型测试时<code>batch_norm</code>层的use_global_stats设为false居然没影响????  错觉</li><li>训练过程开始良好, 中途出现后方部分卷积开始死亡(参数值非常低), 然后向前传染, 大部分卷积死亡, 表现为验证集上非常不稳定<ul><li>推测是ReLU死亡</li></ul></li><li>caffe 和 opencv 一起 import 会出错<ul><li>added <code>-Wl,-Bstatic -lprotobuf -Wl,-Bdynamic</code> to <code>LDFLAGS</code> and removed <code>protobuf</code> from <code>LIBRARIES</code> ( 参照 <a href="https://github.com/BVLC/caffe/issues/1917" target="_blank" rel="noopener">https://github.com/BVLC/caffe/issues/1917</a> )</li></ul></li></ol><h4 id="犯2记录"><a href="#犯2记录" class="headerlink" title="犯2记录"></a>犯2记录</h4><ol><li><code>resize</code>层或者叫<code>upsample</code> <code>upscale</code> 层, 若训练时使用的缩放算法不同, 在卷积到比较小的时候(4x4)之类的, 会由于策略差异导致缩放前后误差非差大</li><li>test 或 upgrade 时 model 和 prototxt 写反<blockquote><p>[libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 2:1: Invalid control characters encountered in text.<br>…..<br><strong><em> Check failure stack trace: </em></strong><br>已放弃 (核心已转储)</p></blockquote></li><li>二分类问题 SoftmaxWithLoss 层不要设 ignore_label, ignore_label 是会忽略该 label 的 loss 和 diff 传递, 导致结果会完全倒向另一个 label , 因为 SoftmaxWithLoss 是计算准确率来算 loss 的</li></ol><h4 id="常见安装问题"><a href="#常见安装问题" class="headerlink" title="常见安装问题"></a>常见安装问题</h4><ol><li><p>一般常见 protobuf 问题, 因为 Tensorflow 也用 protobuf, 不仅用, 还会自动升级 protobuf, 而 caffe 不怎么支持新版本的 protobuf, 所以如果配置了其他开源库的开发环境之后 caffe 报错了, 基本可以从几个方面检查 protobuf 有没问题.</p><ul><li><code>pip list</code>, 查看 protobuf 版本, 一般 2.6.1 比较通用, 如果是 3.5 那就换吧. 如果同时使用了 python2.7 和 python 3.5 的话那还要注意 pip 也分 pip2 和 pip3, 安装的库也分别独立. 可以在 <code>/usr/bin</code>, <code>/usr/local/bin</code>, <code>/$HOME/.local/bin</code> 下找到 pip 脚本, 打开就能看到它用的是 python2.7 还是 python3.5. ( <em>然后出现了下一个问题</em> )</li><li><code>protoc --version</code>, protobuf 依赖的东西, 查看它的版本和 protobuf 的是否一样, 不一样的话可以通过下载相应版本 release, 或者从源码安装 protobuf. 然后在 <code>/etc/ld.so.conf</code> 里面添加上一行 <code>/usr/local/lib</code>, 然后 <code>sudo ldconfig</code> 更新下链接库就行了. ( <em>然后出现了下一个问题</em> )</li><li><code>apt list | grep &quot;protobuf&quot;</code>, 有时候会有用 <code>apt-get install</code> 和 <code>pip install</code> 装了两种不同版本的 protobuf 的情况, 这时候可以 <code>apt</code> 删除并重新安装 protobuf ( <em>然后出现了下一个问题</em> )</li><li><code>File already exists in database: caffe.proto</code>, 库链接问题或者版本问题 ( 2.6.1 不好用 ), <code>pip uninstall protobuf</code> 删掉 protobuf, 重启, 加 -fPIC 到 configure, 然后 <code>./configure --disable-shared</code>, 然后在 protobuf 3.3 版本下 <code>cd $PROTOBUF_BUILD_DIR/python</code>, <code>python setup.py build</code>, <code>python setup.py test</code>, <code>python setup.py install</code> ( <em>然而出现了下一个问题</em> )<ul><li>还可能是 caffe 玄学问题, 总之最简单的就是直接把能用的 caffe 替换过来</li></ul></li><li><code>make all</code> 时出现一堆 protobuf 未定义的引用问题. ( <em>未解, 回溯 2.6.1</em> )</li><li><p>2.6.1:</p><ul><li><code>caffe_pb2.py: syntax error</code>, 注释掉默认 caffe 的 <code>python/caffe/proto/caffe_pb2.py</code>, 至于为什么项目 caffe 没有用自己的 <code>caffe_pb2.py</code> 而用到默认 caffe, 是因为没有成功 <code>make pycaffe</code> ??? 总之应该是版本问题.</li><li><p><code>File already exists in database: caffe.proto</code> 依旧存在这个问题, 在 <code>import caffe</code> 后 <code>import cv2</code> 会发生, 还是需要静态链接 protobuf, 这样可以解决:</p><ul><li><blockquote><p>linking caffe against libprotobuf.a instead of libprotobuf.so could solve this issue</p></blockquote></li><li><blockquote><p>I changed caffe’s Makefile. Specifically, I added -Wl,-Bstatic -lprotobuf -Wl,-Bdynamic to LDFLAGS and removed protobuf from LIBRARIES.<br>I have uploaded my Makefile to gist (<a href="https://gist.github.com/tianzhi0549/773c8dbc383c0cb80e7b)" target="_blank" rel="noopener">https://gist.github.com/tianzhi0549/773c8dbc383c0cb80e7b)</a>. You could check it out to see what changes I made (Line 172 and 369).</p></blockquote></li></ul></li><li><p><code>File &quot;/usr/lib/python2.7/dist-packages/caffe/pycaffe.py&quot;, line 13, in &lt;module&gt;from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver,libcaffe.so.1.0.0: cannot open shared object file: No such file or directory</code>. 这是 python 又喵了咪了用了默认 release 版 caffe, 删掉 <code>/usr/lib/python2.7/dist-packages/caffe</code>, 然后在工程头处 <code>import sys</code> 加<code>sys.path.insert(&#39;/home/sad/ENet/caffe-enet/python&#39;)</code> 和 <code>sys.path.insert(&#39;/home/sad/ENet/caffe-enet/python/caffe&#39;)</code> 再 <code>import caffe</code>, 问题终于解决!</p></li></ul></li></ul></li><li><p><code>libcudnn.so.5: cannot open shared object file: No such file or directory</code>, ld 抽风, 需要专门刷新下 cuda 链接路径 :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ldconfig /usr/local/cuda-8.0/lib64</span><br></pre></td></tr></table></figure></li><li><p><code>*** SIGSEGV (@0x100000049) received by PID 703 (TID 0x7f52cbb1c9c0) from PID 73; stack trace: ***</code> 或者 <code>Segmentation fault (core dumped)</code>, 可能是 python 层的使用出了问题</p></li><li>段错误, <code>import caffe</code> 退出后错误, 有可能是用了 opencv contrib 的 <code>LIBRARY</code>, 在 <code>Makefile</code> 里删掉 <code>opencv_videoc</code> 什么的…</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以往在使用 caffe 中遇到的部分问题记录。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术经验" scheme="https://lamply.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BB%8F%E9%AA%8C/"/>
    
      <category term="问题记录" scheme="https://lamply.github.io/tags/%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
      <category term="caffe" scheme="https://lamply.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>深度学习方法的人脸对齐</title>
    <link href="https://lamply.github.io/2018/07/19/face-alignment-cnn/"/>
    <id>https://lamply.github.io/2018/07/19/face-alignment-cnn/</id>
    <published>2018-07-19T04:40:30.000Z</published>
    <updated>2018-08-06T06:50:29.341Z</updated>
    
    <content type="html"><![CDATA[<figure align="center"><br><img src="/2018/07/19/face-alignment-cnn/1.png" alt="1"><br></figure><p>这部分是去年 9 月份开始的工作，算是第一次真正踏入深度学习的领域。具体工作也还算简单，就是复现一篇深度学习方法做的人脸对齐，当练练手。<br><a id="more"></a></p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>因为深度学习的发展，很多传统的计算机视觉技术有了突破性进展，市面上也涌现了不少以前技术无法做到的产品，传统的像人脸检测、人脸对齐方面也有很大进步。这里就谈谈其中的一个，Deep Alignment Network [1]（下面简写 DAN）。  </p><p>DAN 是用卷积神经网络做人脸对齐的工作，大致思想就是级联卷积神经网络，每阶段都包含前一阶段的输出作为输入，输出 bias，加上 bias 并摆正人脸关键点和输入图，用 输出点生成的 heatmap + 最后一层卷积输出的特征 reshape 图 + 摆正后的原图 作为下一阶段的输入。这样就能不断修正，以达到 robust 的结果。  </p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>作者在 GitHub 上开源了代码 [2]，用的是 Theano 实现。除了验证集设置、initshape 部分冗余 和 测试的部分代码 外，其他部分应该都是没问题的，直接训练得到的结果除了 Challenging subset 稍微要差一些外，其他都和论文一致，算是比较好复现的一个了。  </p><p>我用的是 caffe 做复现，一方面是方便部署到安卓，另一方面是简单好用，改起来也容易。当时还没有 TensorFlow Lite，从各个方面来说 TensorFlow 都不太方便。当然，现在 TensorFlow 就更厉害了。  </p><p>大体上要做的事就是先实现一阶段，写好方便训练、测试用的 python 代码，把数据集封装成 hdf5。因为一阶段没用到自定义层，所以直接写出网络结构的 prototxt 和 solver 就能训练了，训练好后就能作为二阶的 pre-trained model。当然一阶相当于直接 VGG + 回归输出，所以也可以直接看到效果了，我训练出来测试结果如下（测试方法对应代码里的 centers，也就是 inter-pupil normalization error）:  </p><table><thead><tr><th style="text-align:center">Full set (%)</th><th style="text-align:center">Common subset (%)</th><th style="text-align:center">Challenging subset (%)</th></tr></thead><tbody><tr><td style="text-align:center">6.09</td><td style="text-align:center">5.29</td><td style="text-align:center">9.37</td></tr></tbody></table><p>因为训练过程稍有不同，参数也没怎么调，而且后面发现 heatmap 有一点小问题，这个结果和原代码一阶训练的结果有些差异（AUC 差大约 3%），不过无妨，这个结果已经比传统的方法要强得多了，我们继续二阶训练。  </p><p>二阶大部分代码可以和一阶共用，主要要做的部分就是把论文提到的几个自定义层实现，对应这四个地方：</p><ol><li>根据第一阶预测的结果和 mean shape 对比求出仿射变换参数</li><li>根据仿射变换参数对输入图做仿射变换，也就是对正原图啦</li><li>根据仿射变换参数对第一阶预测的结果做仿射变换，当然还要包括反变换的实现</li><li>根据对正的一阶预测结果产生 heatmap  </li></ol><p>然后还有一些 caffe 不支持的又比较常用的层，也就是 resize 层（也有叫 Interp 层或者 upsample 层，都是做插值，我个人认为最好用和部署框架相同的算法）。还有 loss 层，这个会影响到测试的结果和实际效果，我用的是和测试方法一致的度量来做 loss。  </p><p>写好这些层的代码后还有两件事要做。一是单独测试每一层的输出，确保每层前向都各自没问题；二是要做 gradient check，保证反向传播的梯度数值正确。  </p><p>完成一切之后，用一阶段模型作 pre-trained model，进行训练：</p><div align="center"><br><img src="/2018/07/19/face-alignment-cnn/3.jpg" width="80%"><br>训练过程<br></div><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>最终结果：  </p><table><thead><tr><th style="text-align:center">Full set (%)</th><th style="text-align:center">Common subset (%)</th><th style="text-align:center">Challenging subset (%)</th></tr></thead><tbody><tr><td style="text-align:center">5.02</td><td style="text-align:center">4.30</td><td style="text-align:center">7.95</td></tr></tbody></table><p>可以看到和论文结果已经很接近了，这个任务也就大致完成了。比较遗憾的是这个网络不太好替换，后来我尝试把 backbone 从 VGG 更换成其他的轻量型网络，效果都不太理想，而且一到二阶段时由于三张原尺寸图 concat 做输入导致网络参数和运算量剧增也是一个很大的问题。另外，训练过程也可以看到存在非常大的过拟合。虽然有很多地方可以改进，不过毕竟不是首要的研发项目，所以后面就没有做下去了。</p><p>整个网络的结构框图如下：</p><figure align="center"><br><img src="/2018/07/19/face-alignment-cnn/2.png" width="60%"><br></figure><p>[参考文献]:<br>[1] <a href="https://arxiv.org/pdf/1706.01789.pdf" target="_blank" rel="noopener">《Deep Alignment Network: A convolutional neural network for robust face alignment》</a><br>[2] github: <a href="https://github.com/MarekKowalski/DeepAlignmentNetwork" target="_blank" rel="noopener">MarekKowalski/DeepAlignmentNetwork</a>  </p>]]></content>
    
    <summary type="html">
    
      &lt;figure align=&quot;center&quot;&gt;&lt;br&gt;&lt;img src=&quot;/2018/07/19/face-alignment-cnn/1.png&quot; alt=&quot;1&quot;&gt;&lt;br&gt;&lt;/figure&gt;

&lt;p&gt;这部分是去年 9 月份开始的工作，算是第一次真正踏入深度学习的领域。具体工作也还算简单，就是复现一篇深度学习方法做的人脸对齐，当练练手。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术经验" scheme="https://lamply.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BB%8F%E9%AA%8C/"/>
    
      <category term="人脸对齐" scheme="https://lamply.github.io/tags/%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/"/>
    
  </entry>
  
  <entry>
    <title>传统方法的人脸对齐</title>
    <link href="https://lamply.github.io/2018/07/15/face-alignment/"/>
    <id>https://lamply.github.io/2018/07/15/face-alignment/</id>
    <published>2018-07-15T04:11:12.000Z</published>
    <updated>2018-08-06T05:50:18.317Z</updated>
    
    <content type="html"><![CDATA[<p><div align="center"><br><img src="/2018/07/15/face-alignment/1.png" alt="1"><br></div><br>尽管是在复习备战阶段，不过还是要时常回顾一下之前工作的经验，不能太过生疏。<br>这里是关于应用传统方法做人脸对齐的经验总结，是在去年5月到7月的工作，也是我入职后的第一个正式项目，用的是 SDM (Supervised Descent Method) [1] 的方法，具体细节可能不太记得，所以会慢慢补完。<br><a id="more"></a></p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>在深度学习杀到这领域前，有两种主流的人脸对齐方法，一个是14年的号称能达到 3000FPS 的 LBF，还有一种就是13年的 SDM。由于 3000fps 复现效果不理想，实际上 SDM 比起 3000fps 精度要高一些，而且还有不错的现成代码，当时也只是想把静态的人脸对齐做了，所以就选用了 SDM。 </p><p>关于 SDM，其实是作者提出的一种非线性最小二乘优化的方法，类似牛顿步等方法，只不过回避了需要大计算量的 Hessian 矩阵和 Jacobian 矩阵的计算，人脸对齐算是它的一种应用。论文中的大致意思就是从牛顿步出发想办法把那两矩阵裹起来改为用迭代回归来学习，得到下降方向和大小，关于它更多的理论理解部分在官网上有很简单直观的介绍，这里暂时先放着。  </p><h3 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h3><p>我用的是 github 上 patrikhuber[2] 的开源代码，这个版本比论文的 SDM 稍微改进了一些，不过当然无论是模型大小、运算速度还是算法效果，都远不足以应用。所以我花了不少时间看论文，并在源代码基础上实现了一大堆算法模块，然后不停训练。当然很多想法实际上并没有奏效。 </p><p>那段时间现在看起来是挺盲目的，想到什么加什么，甚至还纠结用 HOG 还是 DSIFT 做特征提取，还是两者混合 SDM 迭代时切换特征提取器。在诸如此类想法上花了不少工程上的功夫以达到能够随时切换配置做训练，更重要的是浪费了训练时间。实际上这些都不是制约最终效果的瓶颈，因为当时的特征提取的范围是限定的，瓶颈并不在于特征提取器本身，况且 HOG 和 DSIFT 本身差别并不大，在瓶颈时如果不找出瓶颈下功夫而着眼于其他不确定的想法往往是不明智的。  </p><p>其实那时候很多的想法可能并不是没有奏效，而是被瓶颈掩盖了，造成了想法无效的错觉。换句话说，我们做算法的常常要评判什么想法是有效的，什么想法是无效的，而我觉得这种评判是要有所保留的，可能之所以想法无效是因为被什么我们没看到的因素所制约了，暂时行不通罢了。  </p><p>扯远了，总之当时的瞎蒙乱撞最终幸运的还是找到了几个有用的改进方法。类似《Extended Supervised Descent Method for Robust Face Alignment》[3] 里提到的，一个是关于特征提取范围以及cell数量的改进，改成了顺应 SDM 迭代过程从大到小的范围、从粗到细的计算；第二个就是分开全局和局部来进行回归。这一些改进其实算是人脸对齐中比较常见的改进了，没有太多新意，不过对于全局和局部区分回归来说，比起对最终效果的改进，这个方法对模型大小和运算速度的改进更为明显。最后为了工程上的应用，这些方法都需要大量调参。  </p><p>完成上面的这些其实也提升不了太多，关于这个项目最终提升最大的还是加入了人脸检测时得到的 5 个点做先验。也就是把 5 点扩展到 68 点，再加点 trick 使这 68 点极其接近 ground truth，最后作为 init shape 输入到 SDM。这个方法让人脸对齐准确率和成功率大大增加，因为先验降低了回归的难度，把瓶颈推到了人脸检测时的五点回归成功率。虽然某些特殊情况可能会因为先验产生一些误导，不过也让通常情况下的对齐准确度达到非常高的地步，这是值得的。（后来我记得找到了一篇2017年7月的论文也有提到用类似我这个方法的改进版 SDM 和 LBF 来和他的深度学习方法对比的论文，只不过他用的是JDA，我用的是MTCNN，还有就是生成的 init shape 方法略有不同。）  </p><h3 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h3><p>至此，我的 SDM 人脸对齐的效果已经和当时竞品水平相当了，最终在 300w 上测试的结果为（用的 inter-pupil normalization error）  </p><table><thead><tr><th style="text-align:center">Full set (%)</th><th style="text-align:center">Common subset (%)</th><th style="text-align:center">Challenging subset (%)</th></tr></thead><tbody><tr><td style="text-align:center">4.75</td><td style="text-align:center">4.09</td><td style="text-align:center">7.47</td></tr></tbody></table><p>当然了，因为这个测试数字是用了五点 ground truth 做抖动的前提下得到的，所以没严格的参考价值，无法和其他公开方法做比较，不过也能反映这种方法的有效性。   </p><p>最终做了三个模型，分别是有五点先验的 68 点、106 点，以及没有五点先验的 106 点。模型大小都是 5MB 左右， 在小米 mix2 上速度 20ms 左右。demo 效果大致如下：</p><figure align="center"><br><img src="/2018/07/15/face-alignment/2.png" width="80%"><br></figure>  <h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>关于训练数据集其实还有非常多要写，比如我这里只用了 300-w 数据集，在这基础上翻转、模糊、调对比度，还有关于如何将 68 点数据集扩增到 106 点的，也参考复现了几篇论文。还有的话就是安卓端的部署，包括算法的移植、接口封装，运算速度的优化，模型的压缩等等。之后有空的话可能会整理补上。</p><p>[参考文献]:<br>[1] <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Xiong_Supervised_Descent_Method_2013_CVPR_paper.pdf" target="_blank" rel="noopener">《Supervised Descent Method and its Applications to Face Alignment》</a><br>[2] github: <a href="https://github.com/patrikhuber/superviseddescent" target="_blank" rel="noopener">patrikhuber/superviseddescent</a><br>[3] <a href="http://pdfs.semanticscholar.org/5c82/0e47981d21c9dddde8d2f8020146e600368f.pdf" target="_blank" rel="noopener">《Extended Supervised Descent Method for Robust Face Alignment》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;div align=&quot;center&quot;&gt;&lt;br&gt;&lt;img src=&quot;/2018/07/15/face-alignment/1.png&quot; alt=&quot;1&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;br&gt;尽管是在复习备战阶段，不过还是要时常回顾一下之前工作的经验，不能太过生疏。&lt;br&gt;这里是关于应用传统方法做人脸对齐的经验总结，是在去年5月到7月的工作，也是我入职后的第一个正式项目，用的是 SDM (Supervised Descent Method) [1] 的方法，具体细节可能不太记得，所以会慢慢补完。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术经验" scheme="https://lamply.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BB%8F%E9%AA%8C/"/>
    
      <category term="人脸对齐" scheme="https://lamply.github.io/tags/%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/"/>
    
  </entry>
  
  <entry>
    <title>《哥德尔·艾舍尔·巴赫——集异璧之大成》其四</title>
    <link href="https://lamply.github.io/2018/07/11/%E4%B8%80%E8%87%B4%E6%80%A7%E5%AE%8C%E5%85%A8%E6%80%A7%E4%B8%8E%E5%87%A0%E4%BD%95%E5%AD%A6/"/>
    <id>https://lamply.github.io/2018/07/11/一致性完全性与几何学/</id>
    <published>2018-07-11T08:14:00.000Z</published>
    <updated>2018-07-25T07:40:38.910Z</updated>
    
    <content type="html"><![CDATA[<p>本文是 GEB 的第四章部分阅读笔记，这章开始深入对形式系统的意义起效做讨论，介绍了一致性、完全性，还有用于阐述未定义项的几何发展史。<br><a id="more"></a></p><h3 id="引论"><a href="#引论" class="headerlink" title="引论"></a>引论</h3><p>前面部分看着挺容易理解，不过不太好总结，主要是通过对位藏头诗来讲述意义的层次，即显明的意义和隐含的意义。而各层次的意义通过同构来传达。最后引出哥德尔定理，这是对位藏头诗的主要目的。  </p><p>这个对位藏头诗例子不太亲切，主要是关于唱机的唱针沿唱片纹道颤出声音，外界的所有声音和唱机本身也震颤，那总有一种唱片可以让唱机因为震颤而自毁。除此之外还有非常多隐藏的同构，就不列举了。  </p><p>接着介绍了不完全性，即真理超出了形式系统的定理资格的规定。然后深入形式系统讨论 真理 和 定理资格。  </p><p>这里举了个例子，把 pq 系统稍改一下，加入一条公理模式：若 x 是一个短杠串，则 xqxp- 是一个公理，那么之前的pq系统的解释就不能完全应用了，况且新旧定理存在不一致，比如 “–q-p-“ 和 “-q-p-“ 都是公理。为了重新获得一致性，我们只要把 q 的解释改为小于或等于就可以了。  </p><h3 id="几何发展史"><a href="#几何发展史" class="headerlink" title="几何发展史"></a>几何发展史</h3><p>通过刚才修改pq系统的例子，作者引出了对几何发展的历史的探讨。  </p><p>首先是欧几里得通过《几何原理》将当时的几何知识从最简单的概念和定义开始逐渐地、精细地建立成庞大的几何学体系。而所有的这些都只基于五条公设：  </p><ol><li>一条直线段可以联接两个点。</li><li>一条直线上任何一条直线段可以无限延伸。</li><li>给定一条直线段，可以以一个端点为圆心，以此线段为半径做一个圆。</li><li>一切直角都彼此相等。</li><li>如果两条直线与第三条直线相交时，在第三条直线的某一侧三条线所夹的内角之和小于两个直角的和，则那两条直线沿着这一侧延伸足够长之后必然相交。</li></ol><p>可以看到第五条公设不那么优雅，只是没有办法证明，所以只能设定为公设。（现在称没有第五条公设帮助的几何部分为绝对几何，额外满足第五公设的就是欧式几何，不满足第五公设而满足另外一个公设的就是非欧几何）  </p><p>后继者们为了证明第五公设花了很多力气，不过都是错的。其中有意思的是济罗拉莫·萨彻利曾通过反证得出一个“与直线的本质相抵触”的命题，不过他找到这个矛盾过后就不干了，这让他与后来“双曲几何学”失之交臂。而五十年过后，兰伯特也重复了这种失之交臂。直到再过了四十年，非欧几何被认可了。  </p><p>1823年非欧几何同时被J.鲍耶和罗巴切夫斯基发现，有意思的是同一年法国数学大家勒让德在相当程度上沿着萨彻利的路子想出了第五公设的一个证明。尽管其他人研究了很久一直都没有结果，但非欧几何的出现在那个时候是有奇妙的同时性的。这种科学发现的同时性是挺有趣的一个现象。  </p><h3 id="一致性、完全性"><a href="#一致性、完全性" class="headerlink" title="一致性、完全性"></a>一致性、完全性</h3><p>非欧几何的发现带来了一种启示，就是未定义项的问题。大体来说，得到“与直线的本质相抵触”的结果，是因为没有摆脱先入为主的“直线”概念。如果把它们当成未定义项，仅由公设来给它们定义，那就能得到不同的结果。这就跟之前 pq 系统中我们修改了 q 的解释一样，摆脱了先入为主的观念，从满足新的定理上重新找到了 q 的正确解释。  </p><p>（待续）</p><p><br></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是 GEB 的第四章部分阅读笔记，这章开始深入对形式系统的意义起效做讨论，介绍了一致性、完全性，还有用于阐述未定义项的几何发展史。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书笔记" scheme="https://lamply.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《哥德尔·艾舍尔·巴赫——集异璧之大成》其三</title>
    <link href="https://lamply.github.io/2018/06/28/%E5%9B%BE%E5%BD%A2%E4%B8%8E%E8%A1%AC%E5%BA%95/"/>
    <id>https://lamply.github.io/2018/06/28/图形与衬底/</id>
    <published>2018-06-27T16:00:00.000Z</published>
    <updated>2018-07-11T06:05:18.663Z</updated>
    
    <content type="html"><![CDATA[<p>本文是 GEB 的第三章部分阅读笔记，内容多是我自己筛选、压缩、重排的，并尽可能的保留书中名词的译名和说法。因为我只有非数学系普通理工科大学生的数学基础，不怎么熟悉数论逻辑学什么的，所以可能会有某些错误的理解或说法，就酱。<br><a id="more"></a></p><h3 id="素数系统"><a href="#素数系统" class="headerlink" title="素数系统"></a>素数系统</h3><p>这部分先是引入描述素数的形式系统，然后试图通过构建其他更简单的类似形式系统来达到目的。<br>比如书中使用了一个看上去很聪明的方法：  </p><ol><li>构建tq系统（类似pq系统的乘法版本）</li><li>制定Cx定理（如果 xqy-tz- 是定理，则Cx是定理）来刻划合数（因为y- z- 为数量大于一的短杠）</li><li>草拟一个规则（如果Cx不是一个定理，则Px是个定理）来刻划素数  </li></ol><p>这样看上去似乎就完成了，然而这里面有一个重大缺陷：Cx是否是一个定理不是能够明显表达的印符操作，就像 WJU 系统中 WU 是不是定理没办法一下子看出来一样，我们必须要到形式系统之外才能推出什么什么不可能出现在定理中， 这是不符合被允许的印符操作的。</p><p>这里有个很关键的事情，就是当我们以 惟方式（W） 来工作时，我们很经常会混淆符号串与符号串的解释，把 — 与 3 做同等对待，从而得出一些错误的结论。之前对形式化的要求就显得额外重要，我们不能把算术事实与印符定理相混淆。  </p><p>从这里书中开始谈论非定理的形式问题，我们可以知道定理都是具有共同“形式”的，即Cx，当x为合数数量的短杠时Cx为定理，那对应的非定理是否也是有同一种“形式”呢？答案是对，也不对（这里书中排除了非定理中非良构的符号串，比如 tt-Cqq 这种乱七八糟的东西），无可否认它们都具有某种印符特性，但这是否能称为“形式”确是不清楚的，因为它们是以否定的方式定义的。  </p><p><strong><strong>（题外话：可能会有人跳出来说，Cx规则与合数同构，因为合数反过来就是素数，所以Cx的良构非定理就和素数同构啦。虽然书中并没有对这个做论证，但我想大概就像之前说得那样，这种惟方式是不能用于替换推导的，后面可能就会有完全符合形式系统的方式来得出这种结论。）</strong></strong><br><br></p><h3 id="图形与衬底"><a href="#图形与衬底" class="headerlink" title="图形与衬底"></a>图形与衬底</h3><p><em>到这里书中跳到了另一个层次，讲述刚才的否定在其他层面的形象体现，并通过类比（Analogy，传说中 GEB 的核心）把这种否定观念代回到形式系统中。到这里就能明白之前阿基里斯和乌龟电话里的意思了，虽然我早就看出谜底是蜡烛，不过现在看来乌龟的图形与衬底的提示其实就是映射了“虫虫”谜题和“昔火”谜题，说明这章的主旨正是这种正反相衬。</em><br><br><br>这部分主要介绍了图形和衬底，一般来说，我们看到一幅画，会关注它的图形，也可以叫前景、正空间，不可避免的会产生衬底，也叫背景、负空间。那么除了一般的只有图形有意义，衬底只是附带的，那这幅画书中称作流畅可画出；如果图形和衬底都有意义，就叫倍流畅，就是双倍流畅的意思。埃舍尔就很擅长倍流畅的画，整幅画没有附带的部分，各个区域都是有意义的画像。  </p><p>然后在音乐中也存在这种现象，对应的是旋律与伴奏，还有音符落在强半拍和弱半拍的交织。  </p><p>回到形式系统，我们的目标是把作为负空间表示的素数个短杠符号串Cx改成用正空间表示的Px。从图形与衬底包含相同的信息来看，似乎这是一般意义上可行的，但是事实上只是在这个问题上才可行，有这样一个事实：  </p><p>  存在一个形式系统，其负空间（非定理集）不是任何一个形式系统的正空间（定理集）。  </p><p>换个更专业的描述即是，存在非递归的递归可枚举集。这里“递归可枚举”（缩写r.e.）对应艺术上的流畅可画出，简而言之就是可以按照印符规则生成的集合（所有形式的系统定理集）；“递归”则是对应倍流畅，意思是不仅它本身是r.e.，其补集也是r.e.。  </p><p>这就导致了存在一些形式系统，它们没有用印符规则表述的判断过程。  </p><p>关于这个论点的证明，作者没有详细给出，只是把上面的事实“当作信念而接受”。后面以所有图形都是倍流畅的吗来类比所有的集合都是递归的吗，然后完结这一个议题。  </p><p>最后部分给出了生成素数的形式系统，使用不整除和没有因子的规则来形成单向测试，这里不详细记录了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是 GEB 的第三章部分阅读笔记，内容多是我自己筛选、压缩、重排的，并尽可能的保留书中名词的译名和说法。因为我只有非数学系普通理工科大学生的数学基础，不怎么熟悉数论逻辑学什么的，所以可能会有某些错误的理解或说法，就酱。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书笔记" scheme="https://lamply.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《哥德尔·艾舍尔·巴赫——集异璧之大成》其二</title>
    <link href="https://lamply.github.io/2018/06/27/%E5%85%B3%E4%BA%8E%E5%BD%A2%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    <id>https://lamply.github.io/2018/06/27/关于形式系统/</id>
    <published>2018-06-26T16:00:00.000Z</published>
    <updated>2018-07-11T05:58:28.587Z</updated>
    
    <content type="html"><![CDATA[<p>本文是「GEB」第二、三章的读书笔记，这部分是边读边记，同时忽略了一些详细严谨的解释和讨论。<br><a id="more"></a></p><h4 id="pq系统"><a href="#pq系统" class="headerlink" title="pq系统"></a>pq系统</h4><p>首先引入一个简单构造的系统，一个由「p、q、-」三个符合组成的系统，它具有无数条公理，通过公理模式「x为一组短杠”-“，那么”x-qxp-“为一条公理」来产生。其生成规则只有一条「x、y、z为只包含短杠的符号串，若xqypz为定理，则x-qypz-为定理」。  </p><p>因为只包含了加长的规则，所以可以说这个系统存在判定过程。可以找出其判定过程就是：对于任意一个xqypz符号串，x、y、z为仅由短杠组成的符号串，若其数量x=y+z，则xqypz是一个定理。这属于一种自顶向下的判定过程。  </p><p>到这里就涉及到了一个书中的中心问题。可以看到pq的定理和加法的相似，比如 —–q–p— 是一条定理，因为5=2+3。但仔细一想，「—–q–p—是一条定理」是否就和「5=2+3」是一样的呢，我们之所以会这么想，是因为我们在pq定理和加法运算之间看到了「同构」，即两个复杂结构可以互相映射，两者各部分都有两两对应，比如 —– 对应 5，q 对应 =，– 对应 2，p 对应 +，— 对应 3。这种对应关系有一个名称：解释。</p><p>其次，这种对应也存在于经过解释的定理和真陈述（真理）之间，比如刚才的pq定理，尽管它可以用其他猪牛马做解释，但唯有用加法来解释时同构存在于定理和现实的某部分中，所以这种解释是有意义的。  </p><p>值得注意的是，pq定理的加法解释是只有在其符合pq形式系统的形式（即xqypz形式）时才能够与真理同构的，这种形式上的符合称为「良构」。若符号串不是良构的，那就不能为此符号串赋予同等意义的解释，比如 ——–q–p–p–p–，尽管 8=2+2+2+2，但该符号串并不是良构的，所以这不是一条定理，更不能赋予其意义。在这个基础上可以看到，形式系统的意义一定是被动的。  </p><p>后面的部分大致是关于将形式系统推广的时候遇到的障碍。要构造一个和真理完全同构的形式系统时，必须要让每一个定理为真理，每一个非定理为“假理”。但当定理集合无穷的情况下，我们怎么知道所有的定理都在这种解释方法下表达了真理？这里书中介绍了抽象理想的数，并引入了“欧几里得定理”的证明过程，这种证明的陈述具有紧密的上下联系，使得大家最终都必须相信看上去并不显然的结论。这个证明过程使用了新的特定的词汇，用于抽象描述数的性质，避开了无穷。  </p><p><br><br>到这里第二章结束。这一部分算是初步介绍了形式系统，从前面的pq系统开始介绍，后面似乎开始泛化，有点难抓住主题，每一小段都是单独一方面的具体描述，我有点云里雾里，只是隐约感受到开始将形式系统完备到能应用的有意义的状态，那些描述趋向于说明一些困难，以及某一些解决的手段。我觉得这里的困难大概就是前面乌龟和阿基里斯的对话那里隐喻的问题，非常地逻辑学和数学……  </p><p>后面几章会构造一个形式系统，并似乎要讨论关于该系统能否理论上达到我们的思维能力的水平。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是「GEB」第二、三章的读书笔记，这部分是边读边记，同时忽略了一些详细严谨的解释和讨论。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书笔记" scheme="https://lamply.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《哥德尔·艾舍尔·巴赫——集异璧之大成》其一</title>
    <link href="https://lamply.github.io/2018/06/21/WU%E8%B0%9C%E9%A2%98/"/>
    <id>https://lamply.github.io/2018/06/21/WU谜题/</id>
    <published>2018-06-20T16:00:00.000Z</published>
    <updated>2018-07-11T06:00:34.799Z</updated>
    
    <content type="html"><![CDATA[<p>学习之余看起了以前没看完的书，顺便把博客环境重新搭了起来。<br><a id="more"></a></p><h3 id="WJU-谜题"><a href="#WJU-谜题" class="headerlink" title="WJU 谜题"></a>WJU 谜题</h3><p>这是在「GEB」里出现的一个谜题，原版书中是MU谜题，不过意思一样，讲的是通过在一个只包含WJU三个字符串的形式系统内给出WJ字符串要通过四条规则来产生WU字符串的谜题，这四条规则是：  </p><ol><li>如果字符串以J结尾则可以在其后加上一个U，即WUJ -&gt; WUJU</li><li>字符串  Wx 可以扩展为 Wxx，其中「x」为W后面的所有字符串，比如 WJ -&gt; WJJ, WUJ -&gt; WUJUJ</li><li>连续的三个J可以替换成一个U，即 JJJ -&gt; U，但一个U不能替换成三个J</li><li>连续的两个U可以剔除  </li></ol><p>整个系统产生的字符串都是顺序相关的，WJU与WUJ是不同的字符串。那么给定WJ字符串能否转换成WU呢？  </p><p>说实话，这章看得我一愣一愣的，因为这个谜题实际上是不可解的，J只能通过倍增以及三倍加减来变换，即含有质数2，这样得出的J的数量是不可能为3所整除的（假设U与3倍J等价），而书中以此谜题为例子后面却没有对这个谜题做出解答，转而阐述人类智能在处理形式系统时和机器的差异以及其他东西。我觉得如果能稍微提示下花个十来分钟做尝试，然后最后给出答案，再进行深入的探讨会合适很多。而且后面关于判定过程的部分翻译也有些错误和含糊，看得一愣一愣的就突然结束了话题。  </p><p>只能说这本书的写作风格不太符合一般的读书方式，好在大部分的意味都能理解到。</p><h5 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h5><p>提出该WJU系统的意义在于阐述对于一个形式系统的三种处理方式：书中称之为惟方式（W），机方式（J），无方式（U）。也就是分别对应从规则入手，在形式系统外审视其规则做出总结归纳，以及完全按照形式系统的规则像机器一样做运算。</p><p>[参考资料]<br>wiki： <a href="https://en.m.wikipedia.org/wiki/MU_puzzle" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/MU_puzzle</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习之余看起了以前没看完的书，顺便把博客环境重新搭了起来。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书笔记" scheme="https://lamply.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>零核现象</title>
    <link href="https://lamply.github.io/2018/04/19/zero-kernels/"/>
    <id>https://lamply.github.io/2018/04/19/zero-kernels/</id>
    <published>2018-04-19T11:17:12.000Z</published>
    <updated>2018-07-11T05:54:49.934Z</updated>
    
    <content type="html"><![CDATA[<p><div align="center"><br><img src="/2018/04/19/zero-kernels/TingMengDe.bmp" alt="zero"><br></div><br>这里是对零核现象的观察实验记录.<br>具体来说, 就是在训练卷积神经网络的过程中发现模型中有大量卷积核的 L1 变为 0 的情况, 这里为了方便简称零核现象.<br><a id="more"></a><br>最初遇到这种问题是在 <a href="https://github.com/MarekKowalski/DeepAlignmentNetwork" target="_blank" rel="noopener">DAN</a> 训练时发现的, 当时觉得是太大学习率, ReLU 死亡, 后面降低了 lr 就没出现过了.<br>直到后来做分割观察 <a href="https://github.com/TimoSaemann/ENet" target="_blank" rel="noopener">ENet</a> 的预训练模型时又发现了几百个零核的现象, 而且自己用 ShuffleNet 做的分割网络也出现了非常多零核, 这对模型性能显然是有很大影响的, 所以就下定决心解决这个问题.<br>根据之前 DAN 的经验, 我自然先试了一下降低学习率, 结果没用, 虽然零增长的速度变慢了, 但还是会出现, 而且随着训练过程零核几乎线性增加 ( 像上图那样 ).<br>把每层的零核数作纵坐标, 层数作横坐标, 打印成曲线出来就是这个样子:  </p><p><div align="center"><br><img src="/2018/04/19/zero-kernels/coco_train.png" alt="zero_shuffle"><br>一共 2312 个零核, 简直壮观.<br></div><br><br></p><p>因为零核多集中在 depthwise 卷积上, 所以感觉上可能是由于 depthwise 卷积核太薄, 容易训练时掉坑回不来. 后面在网上也没找到多少关于这个的讨论, 唯一一个是在知乎上 <a href="https://www.zhihu.com/question/265709710/answer/298245276" target="_blank" rel="noopener">关于 MobileNet V2 的回答</a>, 也是差不多的解释, 不过我后来去掉了后面所有的 ReLU, 也是得到了很多空核, 也是个迷.<br>无奈之下开始各种调超参. 一是把 batch size 加大, 讲道理更新得会稳一些, 然而并没有用, 零核依然会出现. 二是换了优化器, 用回朴素的 SGD momentum. 这时神奇的事情发生了, 不管怎么训练, 怎么调大 lr, 调小 batch size, 零核都没有出现了…</p><p><div align="center"><br><img src="/2018/04/19/zero-kernels/adam.png" alt="adam"><br>Adam<br><img src="/2018/04/19/zero-kernels/sgd.png" alt="sgd"><br>SGD<br></div><br>最终对比了几个数据集, 从结果上来看 SGD 版比 Adam 版泛化性更强, 性能在个别数据集上也提升很大, 测试指标的标准差更是明显低于 Adam 版的, 做分割出来的边界也变得更加平滑了. 毕竟 Adam 版零核集聚在高层次上, 泛化方面有所缺陷的也是正常的.  </p><h3 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h3><p>后面有空在 mnist 上做了些实验, 发现优化器中只有 Adam 和 RMSProp 肯定会产生零核, 而用其他 SGD, AdaDelta, AdaGrad 都不会产生零核. 既然如此的话, 似乎是可以从 RMSProp 中找到启示的, 但如果要验证还是得具体分析下更新过程才行, 只能暂时留坑了. </p><h3 id="接续"><a href="#接续" class="headerlink" title="接续"></a>接续</h3><p>好吧, 上一次记录的结果是错误的, 并非只有 Adam 和 RMSProp 肯定会产生零核, 理论上零核的产生依旧是和参数更新的速率密不可分, 所以所有优化方法都可能产生零核. 之所以之前产生错误的结论, 是因为统计零核时采用了 L1 + 阈值 的方法, 而实际上零核表现出来的是并未完全收敛于 0, L1 差均值 10 倍以内, 但相较于其他滤波器而言判别力非常低的情况. 比如下面这种.<br>下面是用 AdaGrad 优化一个普通卷积接 depthwise 卷积重复三次的简单网络, 数据集用的 fashion-mnist, 图为其中两层相邻卷积的可视化, 红橙黄绿蓝靛紫, 代表卷积核的绝对值大小, 左边为普通卷积沿通道绝对值叠加得来, 右边为 depthwise 卷积取绝对值得来.  </p><p><div align="center"><br><img src="/2018/04/19/zero-kernels/bad.png" alt="sgd"><br></div><br>其后两层</p><p><div align="center"><br><img src="/2018/04/19/zero-kernels/bad2.png" alt="sgd"><br></div><br>可以看到, 部分卷积核已经几乎一片红了, 对其上层卷积核也产生了影响. 相比之下, SGD 训练的好的情况:</p><p><div align="center"><br><img src="/2018/04/19/zero-kernels/good.png" alt="sgd"><br></div><br>虽然还无法解明什么, 但至少说明了 depthwise 卷积不太好训练, 通过观察训练过程卷积核的变化, 可以看到 SGD + momentum 相对还是比较平稳的, 尽管有些时候可能也会漏网 ( 实际上之前用 MobileNetV2 做分割在 COCO 上预训练也有出现十几个零核… ). 先到这里, 之后一年的时间因为要专心学习, 所以大概要全面搁置了, 可能会整理记录下之前的项目. 就等之后爬上好的平台再说吧.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;div align=&quot;center&quot;&gt;&lt;br&gt;&lt;img src=&quot;/2018/04/19/zero-kernels/TingMengDe.bmp&quot; alt=&quot;zero&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;br&gt;这里是对零核现象的观察实验记录.&lt;br&gt;具体来说, 就是在训练卷积神经网络的过程中发现模型中有大量卷积核的 L1 变为 0 的情况, 这里为了方便简称零核现象.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="观测" scheme="https://lamply.github.io/tags/%E8%A7%82%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>O_o</title>
    <link href="https://lamply.github.io/2018/04/11/water/"/>
    <id>https://lamply.github.io/2018/04/11/water/</id>
    <published>2018-04-11T11:56:00.000Z</published>
    <updated>2018-07-22T06:13:07.186Z</updated>
    
    <content type="html"><![CDATA[<p>这里是划水专区, 在这里你可以自由发呆.<br>评论区要翻墙.</p><div align="center"><br><img src="/2018/04/11/water/portrait.png" alt="dai"><br></div><br><div align="left"><br><a id="more"></a><br>如要赶上线请点击屏幕右/左上角红色标记 <img src="/2018/04/11/water/close.png" alt="close"><br></div><br><div align="center"><br><img src="/2018/04/11/water/8_clock.jpg" alt="up"><br></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这里是划水专区, 在这里你可以自由发呆.&lt;br&gt;评论区要翻墙.&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;&lt;br&gt;&lt;img src=&quot;/2018/04/11/water/portrait.png&quot; alt=&quot;dai&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;br&gt;&lt;div align=&quot;left&quot;&gt;&lt;br&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="闲聊" scheme="https://lamply.github.io/tags/%E9%97%B2%E8%81%8A/"/>
    
  </entry>
  
</feed>
