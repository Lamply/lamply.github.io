<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>In un&#39;altra vita</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://lamply.github.io/"/>
  <updated>2019-07-23T09:27:35.002Z</updated>
  <id>https://lamply.github.io/</id>
  
  <author>
    <name>Lamply</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>人像分割</title>
    <link href="https://lamply.github.io/2019/04/19/protrait-segmentation/"/>
    <id>https://lamply.github.io/2019/04/19/protrait-segmentation/</id>
    <published>2019-04-19T09:00:06.000Z</published>
    <updated>2019-07-23T09:27:35.002Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/2019/04/19/protrait-segmentation/Einstein.png" width="70%">  </div><p>这部分是关于在低计算量下完成人像分割的工作，因为时间充裕，所以调查尝试得比较多，最终完成的效果还不错。<br><a id="more"></a></p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>人像分割（portrait segmentation）属于语义分割的子集，某种程度上类似于只专注人的前景分割，可以看成是二分类的语义分割。不过这里的应用场景是半身肖像，对于效果的评价更加专注于分割边缘和细节的质量，现在看来这次的工作在这方面其实做得比较一般，只是就通常的 IoU 意义上来看还不错。 </p><p>总体来说，人像分割方面的论文比较少，典型的就是 Xiaoyong Shen 等人的工作 [1] [2]。不过实际尝试过程中发现，其中的很多技巧无法有效的应用于低计算量的场景中，固定先验的引入也会导致分割效果在某些情况下变差。  </p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>数据集方面因为我采取了逐步迁移训练，多次 fine-tuning 的方式，所以需要从 ImageNet 到 COCO 人像子集再到标准半身人像分割的数据集。COCO 人像子集是从 COCO 中筛选出含人类（且占面积较大）的部分，大约 4 万张；半身人像分割的数据集训练集一千多张，验证集 50 张，测试集 300 张；还有一些私有的测试集。<br>之所以这样做主要是因为最终用于应用场景的数据集过少，而且从 OSVOS[3] 等文中得到了更多的启发。</p><div align="center"><img src="/2019/04/19/protrait-segmentation/osvos.png">  OSVOS 的做法</div><h4 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h4><p>我主要尝试了 ENet [4]、FCN [5]、FCN dilated convolution 版本 [6]、UNet [7] 的改进版 以及 类 DeepLab V3+ [8] 中的方法。为了保持计算量，除了 [4] 外，我都采用了轻量化网络 ShuffleNet [9] 作为 backbone 来进行复现，大部分网络的输入限制为 224x224。  </p><p>对于 UNet 的改进版，因为是冲着实时而设计的，所以在 decoder 的设计上，尤其是底层特征的引入上显著减少了滤波器数量。当 output stride 为 4 时，可以得到的模型性能为：参数量 79.54K, FLOPS 24.44M, 访存量 46.98MB，可以达到实时。   </p><p>对于类 DeepLab V3+ 的版本，为了尽可能的压缩计算量，我最大限度的利用了 Depthwise Convolution、Dilation 以及 Channel Shuffle 的特性，同时为平衡精度将 output stride 限制为 2（损失非常少），decoder 也改成了更为节省计算量的简单版本，模型性能为：参数量 965.49K, FLOPS 743.37M, 访存量 392.87MB。  </p><blockquote><ul><li><em>来自底层特征的 skip-connection 是有很明显的边缘细化效果的，但同时也十分容易过拟合，这是值得注意的问题。</em> </li><li><em>尽管我把 ASPP 写的很省，但尝试后发现如果那个结构不加 ASPP 的话性能会很差。</em></li><li><em>关于深度，尝试的结果似乎说明了 Encoder 部分需要足够的深（至少 8 个 res block 是不够的），而 Decoder 则不需要太复杂（没有足够的证据，不过增加 2 层能带来的好处不大）</em></li></ul></blockquote><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>训练方面，UNet（实时） 和 类 DeepLab V3+ 方法因为是从头搭起，所以就像上面说的一样，需要两次 fine-tuning，其余方法并没有经过 COCO 人像预训练。优化方法我用的是 SGD+Momentum，事实上这是因为用其他的一些优化方法会导致“零核现象”的发生，我在以前的一篇文章中分析过这点，不知道这个现象和性能的下降有多大关系，但如果用 SGD 的话就不会有这种事情发生，而且性能会更好。</p><blockquote><ul><li><em>COCO 人像部分的预训练的加入很有效，但是必须使用和应用场景十分相似的部分作为预训练才有效，不筛选的话很可能没有效果上的影响。</em>  </li><li><em>使用大 Momentum（我用的 0.99）似乎不错，训练速度会很快而且效果也不会差（实验尝试似乎比 0.9 还要好）。</em></li><li><em>实际训练时还发现有挺严重的过拟合，所以做了些数据集扩增，结果挺有效的。</em>  </li><li><em>在 output stride 为 8 时增加一个辅助损失也是有效的。</em></li></ul></blockquote><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>在 [1] 中给出的数据集中测试结果如下（其中我加入了 Std 度量用于观察分割的稳定性，意为 IoU 的标准差）</p><div class="table-container"><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">mIoU</th><th style="text-align:center">Std</th></tr></thead><tbody><tr><td style="text-align:center">ENet</td><td style="text-align:center">94.04%</td><td style="text-align:center">6.25%</td></tr><tr><td style="text-align:center">FCN</td><td style="text-align:center">95.73%</td><td style="text-align:center">3.10%</td></tr><tr><td style="text-align:center">FCN + dilated</td><td style="text-align:center">96.04%</td><td style="text-align:center">3.27%</td></tr><tr><td style="text-align:center">UNet（实时）</td><td style="text-align:center">95.32%</td><td style="text-align:center">4.03%</td></tr><tr><td style="text-align:center">类 DeepLab V3+</td><td style="text-align:center">96.4%</td><td style="text-align:center">3.25%</td></tr><tr><td style="text-align:center">PortraitFCN+[1]</td><td style="text-align:center">95.91%</td><td style="text-align:center">-</td></tr></tbody></table></div><p>另外我还自己标了个五十多张明暗、运动、背景复杂度变化都比较大的测试图片，在此数据集上测试结果为</p><div class="table-container"><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">mIoU</th><th style="text-align:center">Std</th></tr></thead><tbody><tr><td style="text-align:center">ENet</td><td style="text-align:center">61.42%</td><td style="text-align:center">20.04%</td></tr><tr><td style="text-align:center">FCN</td><td style="text-align:center">87.71%</td><td style="text-align:center">12.01%</td></tr><tr><td style="text-align:center">FCN + dilated</td><td style="text-align:center">89.43%</td><td style="text-align:center">10.62%</td></tr><tr><td style="text-align:center">UNet（实时）</td><td style="text-align:center">91.96%</td><td style="text-align:center">3.95%</td></tr><tr><td style="text-align:center">类 DeepLab V3+</td><td style="text-align:center">93.7%</td><td style="text-align:center">2.3%</td></tr></tbody></table></div><p>可以看到类 DeepLab V3+ 方法测试结果还是挺不错的，其余的多个私有测试集上表现也不错，它在 MIX2 上前向一张图片大概需要 120ms。除此之外，达到实时性能的 UNet 改进版也不错，两者在 P-R 曲线上差异不是很大。</p><div align="center"><img src="/2019/04/19/protrait-segmentation/performance.png" width="80%">  非实时与实时网络的 P-R 曲线比较</div><h2 id="最终的效果"><a href="#最终的效果" class="headerlink" title="最终的效果"></a>最终的效果</h2><div align="center"><img src="/2019/04/19/protrait-segmentation/full.png" width="70%">  </div><p>可以看到边缘的跟踪还是不错的，除此之外，头发的细节处理也可以在顶部图中看到。  </p><p>因为 output stride 为 2 导致分割边缘在放大后加大了间隔，看上去和实际边缘有些距离。在下面的图中可以明显看到这种在人和物件之间的边界、甚至透明物件覆盖影响下算法的处理情况。</p><div align="center"><img src="/2019/04/19/protrait-segmentation/pony.png" width="70%">  </div><p>虽然训练集全都是单人肖像，但多人的情况也能够分割，具体效果还是要看人物所占的尺度等。  </p><div align="center"><img src="/2019/04/19/protrait-segmentation/multi.jpg" width="70%">  </div><p>[参考文献]:<br>[1] <a href="http://xiaoyongshen.me/webpage_portrait/papers/portrait_eg16.pdf" target="_blank" rel="noopener">《Automatic Portrait Segmentation for Image Stylization》</a><br>[2] <a href="https://arxiv.org/pdf/1704.08812.pdf" target="_blank" rel="noopener">《Automatic Real-time Background Cut for Portrait Videos》</a><br>[3] <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Caelles_One-Shot_Video_Object_CVPR_2017_paper.pdf" target="_blank" rel="noopener">《One-Shot Video Object Segmentation》</a><br>[4] <a href="https://arxiv.org/pdf/1606.02147.pdf" target="_blank" rel="noopener">《ENet: A Deep Neural Network Architecture forReal-Time Semantic Segmentation》</a><br>[5] <a href="https://arxiv.org/pdf/1411.4038.pdf" target="_blank" rel="noopener">《Fully Convolutional Networks for Semantic Segmentation》</a><br>[6] <a href="https://arxiv.org/pdf/1702.08502.pdf" target="_blank" rel="noopener">《Understanding Convolution for Semantic Segmentation》</a><br>[7] <a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank" rel="noopener">《U-Net: Convolutional Networks for BiomedicalImage Segmentation》</a><br>[8] <a href="https://arxiv.org/pdf/1802.02611.pdf" target="_blank" rel="noopener">《Encoder-Decoder with Atrous SeparableConvolution for Semantic Image Segmentation》</a><br>[9] <a href="https://arxiv.org/pdf/1707.01083.pdf" target="_blank" rel="noopener">《ShuffleNet: An Extremely Efficient Convolutional Neural Network for MobileDevices》</a>  </p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/2019/04/19/protrait-segmentation/Einstein.png&quot; width=&quot;70%&quot;&gt;  
&lt;/div&gt;

&lt;p&gt;这部分是关于在低计算量下完成人像分割的工作，因为时间充裕，所以调查尝试得比较多，最终完成的效果还不错。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术经验" scheme="https://lamply.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BB%8F%E9%AA%8C/"/>
    
      <category term="语义分割" scheme="https://lamply.github.io/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>关于 LRA 和 Force Regularization 的探索</title>
    <link href="https://lamply.github.io/2018/08/07/Low_Rank_Research/"/>
    <id>https://lamply.github.io/2018/08/07/Low_Rank_Research/</id>
    <published>2018-08-07T05:13:06.000Z</published>
    <updated>2019-05-15T13:10:45.649Z</updated>
    
    <content type="html"><![CDATA[<p>这部分是将《Coordinating Filters for Faster Deep Neural Networks》中提到的 <em>Force Regularization</em> 和 <em>LRA</em> 用于实际项目的效果，虽然现在看来不是很严谨，不过算是一次很好的尝试。<br><a id="more"></a></p><h3 id="设定"><a href="#设定" class="headerlink" title="设定"></a>设定</h3><p>为了探索 <em>Low-Rank Approximations ( LRA )</em> 和 <em>Force Regularization</em>  ( 参考 <em>Wen. “Coordinating Filters for Faster Deep Neural Networks” ICCV 2017</em> ) 在我的工程上的实际效果, 进行了一些实际测试. 由于时间限制, 主要进行了两次探索, 分别为:</p><ul><li>LRA<ul><li>单次大降秩(rank ratio 0.48)</li><li>在大降秩的基础上小降秩(rank ratio 0.8)</li></ul></li><li>LRA + Force Regularization<ul><li>多次迭代 LRA (每次 rank ratio 0.9), FR (0.003 Degradation)</li></ul></li></ul><p>此外还有在 MNIST 上对 LeNet 的对照测试, 结果与文中叙述结论基本一致, Force Regularization 后 LRA 带来的压缩率有进一步的提高, 但主要在全连接层体现 (实验时卷积层个数完全没变), 尚未使用其他网络进行测试, 也没有观察出 Force Regularization 后卷积核的变化, 可能需要进一步实验 (调整 Force Regularization 参数, 用更好的可视化方法 t-SNE 等)</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><ul><li>原模型结果</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">测试</th><th style="text-align:center">分数</th></tr></thead><tbody><tr><td style="text-align:center">原准确率</td><td style="text-align:center">0.8993</td></tr><tr><td style="text-align:center">原召回率</td><td style="text-align:center">0.9017</td></tr><tr><td style="text-align:center">F1-Score</td><td style="text-align:center">0.8919</td></tr></tbody></table></div><ul><li>LRA<br>  单次 0.48 大降秩后 finetune 17300 iters, 接着 0.8 小降秩 finetune 40000 iters:</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">测试</th><th style="text-align:center">0.48分数</th><th style="text-align:center">0.48+0.8分数</th></tr></thead><tbody><tr><td style="text-align:center">准确率</td><td style="text-align:center">0.8947</td><td style="text-align:center">0.9181</td></tr><tr><td style="text-align:center">召回率</td><td style="text-align:center">0.8868</td><td style="text-align:center">0.8157</td></tr><tr><td style="text-align:center">F1-Score</td><td style="text-align:center">0.8813</td><td style="text-align:center">0.8489</td></tr></tbody></table></div><ul><li>LRA + Force Regularization<br>此版本由于原工程正在改进, 所以使用了新的方法 (修改了网络输出层的卷积个数以及输入的通道数, 准确率略微提高, 召回率变化不大)<br>在新方法训练的模型下进行 0.003 Degradation 的 Force Regularization, 400 iters ( 50 iters/epoch ) 后 0.9 rank ratio 降秩, finetune 1000 iters后 继续 0.9 rank ratio 降秩, finetune 1300 iters 后得到收敛结果</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">测试</th><th style="text-align:center">源模型 FR</th><th style="text-align:center">第一次降秩</th><th style="text-align:center">第二次降秩</th></tr></thead><tbody><tr><td style="text-align:center">准确率</td><td style="text-align:center">0.9252</td><td style="text-align:center">0.9207</td><td style="text-align:center">0.9228</td></tr><tr><td style="text-align:center">召回率</td><td style="text-align:center">0.7729</td><td style="text-align:center">0.8520</td><td style="text-align:center">0.8443</td></tr><tr><td style="text-align:center">F1-Score</td><td style="text-align:center">0.8298</td><td style="text-align:center">0.8731</td><td style="text-align:center">0.8698</td></tr></tbody></table></div><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><ol><li>从 LRA 可以看出, 单次大降秩也能恢复到接近源模型的效果(召回率下降大约2%), 模型大小压缩明显(12.7M =&gt; 4.8M), 但是再度降秩模型效果开始较大幅度下降(召回率再度下降7%), 且模型大小变化不大(4.8M =&gt; 3.7M)</li><li>FR 后模型的召回率迅速降低, 但理论上在此基础上再进行多次降秩并最终 finetune 应该是能恢复效果的, 问题 loss 已经几乎收敛, 无法看出有明显下降, 召回率仍然有较大损失, 所以怀疑可能需要降低训练 force regularization, 和 learning rate, 或者有可能是 FR 未足够 finetune 的问题??  </li></ol><h3 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h3><p>进行了FR再测试, 对原模型进行了更久的 finetune, 得到</p><div class="table-container"><table><thead><tr><th style="text-align:center">测试</th><th style="text-align:center">分数</th></tr></thead><tbody><tr><td style="text-align:center">准确率</td><td style="text-align:center">0.9194</td></tr><tr><td style="text-align:center">召回率</td><td style="text-align:center">0.9018</td></tr><tr><td style="text-align:center">F1-Score</td><td style="text-align:center">0.9030</td></tr></tbody></table></div><p>对于速度, 进行了几个小实验, 似乎该方法在小网络上会由于增加卷积层所以减慢前向速度, 而且比起低秩增速, 卷积层的增加带来的负面影响似乎更大. 至少以本项目来说是有些许降速的.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这部分是将《Coordinating Filters for Faster Deep Neural Networks》中提到的 &lt;em&gt;Force Regularization&lt;/em&gt; 和 &lt;em&gt;LRA&lt;/em&gt; 用于实际项目的效果，虽然现在看来不是很严谨，不过算是一次很好的尝试。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术经验" scheme="https://lamply.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BB%8F%E9%AA%8C/"/>
    
      <category term="模型加速和压缩" scheme="https://lamply.github.io/tags/%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F%E5%92%8C%E5%8E%8B%E7%BC%A9/"/>
    
  </entry>
  
  <entry>
    <title>Caffe使用问题记录</title>
    <link href="https://lamply.github.io/2018/08/07/caffe_things/"/>
    <id>https://lamply.github.io/2018/08/07/caffe_things/</id>
    <published>2018-08-07T04:44:35.000Z</published>
    <updated>2018-08-07T04:52:32.934Z</updated>
    
    <content type="html"><![CDATA[<p>以往在使用 caffe 中遇到的部分问题记录。<br><a id="more"></a></p><h4 id="缺陷记录"><a href="#缺陷记录" class="headerlink" title="缺陷记录"></a>缺陷记录</h4><ul><li><code>Xavier</code>初始化没有乘上增益 (ReLU应乘根号2, 等等)</li><li>在matlab上训练得出的模型是col-major,需要将所有矩阵参数转置才能在其他地方用</li><li>老版本caffe在初次前向时会比较慢, 新版未知</li><li>caffe 初始化数据层时启动线程是 <strong>TEST</strong> 和 <strong>TRAIN</strong> 并行进行的, 即使将<code>test_initialization</code>设置为<code>false</code>也会进行一次<strong>TEST</strong>的数据 prefetch,  同样会进行<code>Transform</code>, 所以要注意相关的共享变量.</li><li>BatchNorm 的 eps 默认为 1e-5, 这个数值是 切实 会对结果产生一定影响的, 在 absorb 参数时也要注意</li></ul><h4 id="过程记录"><a href="#过程记录" class="headerlink" title="过程记录"></a>过程记录</h4><ul><li>后向根据<code>top_diff</code>和前向结果算出各<code>blob</code>参数的<code>diff</code>, 以及<code>bottom</code>的<code>diff</code>, 所以分别对<code>blob</code>和<code>bottom</code>求导</li><li>传播时记得不同微分层乘上前面的梯度值<code>top_diff</code>,后传多个梯度值的话全部加起来</li><li><code>setup</code>是在加载网络时调用的, 加载完后不再调用</li></ul><h4 id="错误记录"><a href="#错误记录" class="headerlink" title="错误记录"></a>错误记录</h4><ol><li>Check failed: data_<ul><li>为 <code>blob shape</code> 错误, 一般是 <code>reshape</code> 函数出错, 也可能是网络设计错误导致 <code>shape</code> 传过来时负值错误</li></ul></li></ol><h4 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h4><ol><li>caffe模型测试时<code>batch_norm</code>层的use_global_stats设为false居然没影响????  错觉</li><li>训练过程开始良好, 中途出现后方部分卷积开始死亡(参数值非常低), 然后向前传染, 大部分卷积死亡, 表现为验证集上非常不稳定<ul><li>推测是ReLU死亡</li></ul></li><li>caffe 和 opencv 一起 import 会出错<ul><li>added <code>-Wl,-Bstatic -lprotobuf -Wl,-Bdynamic</code> to <code>LDFLAGS</code> and removed <code>protobuf</code> from <code>LIBRARIES</code> ( 参照 <a href="https://github.com/BVLC/caffe/issues/1917" target="_blank" rel="noopener">https://github.com/BVLC/caffe/issues/1917</a> )</li></ul></li></ol><h4 id="犯2记录"><a href="#犯2记录" class="headerlink" title="犯2记录"></a>犯2记录</h4><ol><li><code>resize</code>层或者叫<code>upsample</code> <code>upscale</code> 层, 若训练时使用的缩放算法不同, 在卷积到比较小的时候(4x4)之类的, 会由于策略差异导致缩放前后误差非差大</li><li>test 或 upgrade 时 model 和 prototxt 写反<blockquote><p>[libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 2:1: Invalid control characters encountered in text.<br>…..<br><strong><em> Check failure stack trace: </em></strong><br>已放弃 (核心已转储)</p></blockquote></li><li>二分类问题 SoftmaxWithLoss 层不要设 ignore_label, ignore_label 是会忽略该 label 的 loss 和 diff 传递, 导致结果会完全倒向另一个 label , 因为 SoftmaxWithLoss 是计算准确率来算 loss 的</li></ol><h4 id="常见安装问题"><a href="#常见安装问题" class="headerlink" title="常见安装问题"></a>常见安装问题</h4><ol><li><p>一般常见 protobuf 问题, 因为 Tensorflow 也用 protobuf, 不仅用, 还会自动升级 protobuf, 而 caffe 不怎么支持新版本的 protobuf, 所以如果配置了其他开源库的开发环境之后 caffe 报错了, 基本可以从几个方面检查 protobuf 有没问题.</p><ul><li><code>pip list</code>, 查看 protobuf 版本, 一般 2.6.1 比较通用, 如果是 3.5 那就换吧. 如果同时使用了 python2.7 和 python 3.5 的话那还要注意 pip 也分 pip2 和 pip3, 安装的库也分别独立. 可以在 <code>/usr/bin</code>, <code>/usr/local/bin</code>, <code>/$HOME/.local/bin</code> 下找到 pip 脚本, 打开就能看到它用的是 python2.7 还是 python3.5. ( <em>然后出现了下一个问题</em> )</li><li><code>protoc --version</code>, protobuf 依赖的东西, 查看它的版本和 protobuf 的是否一样, 不一样的话可以通过下载相应版本 release, 或者从源码安装 protobuf. 然后在 <code>/etc/ld.so.conf</code> 里面添加上一行 <code>/usr/local/lib</code>, 然后 <code>sudo ldconfig</code> 更新下链接库就行了. ( <em>然后出现了下一个问题</em> )</li><li><code>apt list | grep &quot;protobuf&quot;</code>, 有时候会有用 <code>apt-get install</code> 和 <code>pip install</code> 装了两种不同版本的 protobuf 的情况, 这时候可以 <code>apt</code> 删除并重新安装 protobuf ( <em>然后出现了下一个问题</em> )</li><li><code>File already exists in database: caffe.proto</code>, 库链接问题或者版本问题 ( 2.6.1 不好用 ), <code>pip uninstall protobuf</code> 删掉 protobuf, 重启, 加 -fPIC 到 configure, 然后 <code>./configure --disable-shared</code>, 然后在 protobuf 3.3 版本下 <code>cd $PROTOBUF_BUILD_DIR/python</code>, <code>python setup.py build</code>, <code>python setup.py test</code>, <code>python setup.py install</code> ( <em>然而出现了下一个问题</em> )<ul><li>还可能是 caffe 玄学问题, 总之最简单的就是直接把能用的 caffe 替换过来</li></ul></li><li><code>make all</code> 时出现一堆 protobuf 未定义的引用问题. ( <em>未解, 回溯 2.6.1</em> )</li><li><p>2.6.1:</p><ul><li><code>caffe_pb2.py: syntax error</code>, 注释掉默认 caffe 的 <code>python/caffe/proto/caffe_pb2.py</code>, 至于为什么项目 caffe 没有用自己的 <code>caffe_pb2.py</code> 而用到默认 caffe, 是因为没有成功 <code>make pycaffe</code> ??? 总之应该是版本问题.</li><li><p><code>File already exists in database: caffe.proto</code> 依旧存在这个问题, 在 <code>import caffe</code> 后 <code>import cv2</code> 会发生, 还是需要静态链接 protobuf, 这样可以解决:</p><ul><li><blockquote><p>linking caffe against libprotobuf.a instead of libprotobuf.so could solve this issue</p></blockquote></li><li><blockquote><p>I changed caffe’s Makefile. Specifically, I added -Wl,-Bstatic -lprotobuf -Wl,-Bdynamic to LDFLAGS and removed protobuf from LIBRARIES.<br>I have uploaded my Makefile to gist (<a href="https://gist.github.com/tianzhi0549/773c8dbc383c0cb80e7b" target="_blank" rel="noopener">https://gist.github.com/tianzhi0549/773c8dbc383c0cb80e7b</a>). You could check it out to see what changes I made (Line 172 and 369).</p></blockquote></li></ul></li><li><p><code>File &quot;/usr/lib/python2.7/dist-packages/caffe/pycaffe.py&quot;, line 13, in &lt;module&gt;from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver,libcaffe.so.1.0.0: cannot open shared object file: No such file or directory</code>. 这是 python 又喵了咪了用了默认 release 版 caffe, 删掉 <code>/usr/lib/python2.7/dist-packages/caffe</code>, 然后在工程头处 <code>import sys</code> 加<code>sys.path.insert(&#39;/home/sad/ENet/caffe-enet/python&#39;)</code> 和 <code>sys.path.insert(&#39;/home/sad/ENet/caffe-enet/python/caffe&#39;)</code> 再 <code>import caffe</code>, 问题终于解决!</p></li></ul></li></ul></li><li><p><code>libcudnn.so.5: cannot open shared object file: No such file or directory</code>, ld 抽风, 需要专门刷新下 cuda 链接路径 :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ldconfig /usr/local/cuda-8.0/lib64</span><br></pre></td></tr></table></figure></li><li><p><code>*** SIGSEGV (@0x100000049) received by PID 703 (TID 0x7f52cbb1c9c0) from PID 73; stack trace: ***</code> 或者 <code>Segmentation fault (core dumped)</code>, 可能是 python 层的使用出了问题</p></li><li>段错误, <code>import caffe</code> 退出后错误, 有可能是用了 opencv contrib 的 <code>LIBRARY</code>, 在 <code>Makefile</code> 里删掉 <code>opencv_videoc</code> 什么的…</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以往在使用 caffe 中遇到的部分问题记录。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术经验" scheme="https://lamply.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BB%8F%E9%AA%8C/"/>
    
      <category term="问题记录" scheme="https://lamply.github.io/tags/%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
      <category term="caffe" scheme="https://lamply.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>深度学习方法的人脸对齐</title>
    <link href="https://lamply.github.io/2018/07/19/face-alignment-cnn/"/>
    <id>https://lamply.github.io/2018/07/19/face-alignment-cnn/</id>
    <published>2018-07-19T04:40:30.000Z</published>
    <updated>2019-03-05T14:45:39.451Z</updated>
    
    <content type="html"><![CDATA[<figure align="center"><img src="/2018/07/19/face-alignment-cnn/1.png"></figure><p>这部分是去年 9 月份开始的工作，算是第一次真正踏入深度学习的领域。具体工作也还算简单，就是复现一篇深度学习方法做的人脸对齐，当练练手。<br><a id="more"></a></p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>因为深度学习的发展，很多传统的计算机视觉技术有了突破性进展，市面上也涌现了不少以前技术无法做到的产品，传统的像人脸检测、人脸对齐方面也有很大进步。这里就谈谈其中的一个，Deep Alignment Network [1]（下面简写 DAN）。  </p><p>DAN 是用卷积神经网络做人脸对齐的工作，大致思想就是级联卷积神经网络，每阶段都包含前一阶段的输出作为输入，输出 bias，加上 bias 并摆正人脸关键点和输入图，用 输出点生成的 heatmap + 最后一层卷积输出的特征 reshape 图 + 摆正后的原图 作为下一阶段的输入。这样就能不断修正，以达到 robust 的结果。  </p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>作者在 GitHub 上开源了代码 [2]，用的是 Theano 实现。除了验证集设置、initshape 部分冗余 和 测试的部分代码 外，其他部分应该都是没问题的，直接训练得到的结果除了 Challenging subset 稍微要差一些外，其他都和论文一致，算是比较好复现的一个了。  </p><p>我用的是 caffe 做复现，一方面是方便部署到安卓，另一方面是简单好用，改起来也容易。当时还没有 TensorFlow Lite，从各个方面来说 TensorFlow 都不太方便。当然，现在 TensorFlow 就更厉害了。  </p><p>大体上要做的事就是先实现一阶段，写好方便训练、测试用的 python 代码，把数据集封装成 hdf5。因为一阶段没用到自定义层，所以直接写出网络结构的 prototxt 和 solver 就能训练了，训练好后就能作为二阶的 pre-trained model。当然一阶相当于直接 VGG + 回归输出，所以也可以直接看到效果了，我训练出来测试结果如下（测试方法对应代码里的 centers，也就是 inter-pupil normalization error）:  </p><div class="table-container"><table><thead><tr><th style="text-align:center">Full set (%)</th><th style="text-align:center">Common subset (%)</th><th style="text-align:center">Challenging subset (%)</th></tr></thead><tbody><tr><td style="text-align:center">6.09</td><td style="text-align:center">5.29</td><td style="text-align:center">9.37</td></tr></tbody></table></div><p>因为训练过程稍有不同，参数也没怎么调，而且后面发现 heatmap 有一点小问题，这个结果和原代码一阶训练的结果有些差异（AUC 差大约 3%），不过无妨，这个结果已经比传统的方法要强得多了，我们继续二阶训练。  </p><p>二阶大部分代码可以和一阶共用，主要要做的部分就是把论文提到的几个自定义层实现，对应这四个地方：</p><ol><li>根据第一阶预测的结果和 mean shape 对比求出仿射变换参数</li><li>根据仿射变换参数对输入图做仿射变换，也就是对正原图啦</li><li>根据仿射变换参数对第一阶预测的结果做仿射变换，当然还要包括反变换的实现</li><li>根据对正的一阶预测结果产生 heatmap  </li></ol><p>然后还有一些 caffe 不支持的又比较常用的层，也就是 resize 层（也有叫 Interp 层或者 upsample 层，都是做插值，我个人认为最好用和部署框架相同的算法）。还有 loss 层，这个会影响到测试的结果和实际效果，我用的是和测试方法一致的度量来做 loss。  </p><p>写好这些层的代码后还有两件事要做。一是单独测试每一层的输出，确保每层前向都各自没问题；二是要做 gradient check，保证反向传播的梯度数值正确。  </p><p>完成一切之后，用一阶段模型作 pre-trained model，进行训练：</p><div align="center"><img src="/2018/07/19/face-alignment-cnn/3.jpg" width="80%">  训练过程</div><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>最终结果：  </p><div class="table-container"><table><thead><tr><th style="text-align:center">Full set (%)</th><th style="text-align:center">Common subset (%)</th><th style="text-align:center">Challenging subset (%)</th></tr></thead><tbody><tr><td style="text-align:center">5.02</td><td style="text-align:center">4.30</td><td style="text-align:center">7.95</td></tr></tbody></table></div><p>可以看到和论文结果已经很接近了，这个任务也就大致完成了。比较遗憾的是这个网络不太好替换，后来我尝试把 backbone 从 VGG 更换成其他的轻量型网络，效果都不太理想，而且一到二阶段时由于三张原尺寸图 concat 做输入导致网络参数和运算量剧增也是一个很大的问题。另外，训练过程也可以看到存在非常大的过拟合。虽然有很多地方可以改进，不过毕竟不是首要的研发项目，所以后面就没有做下去了。</p><p>整个网络的结构框图如下：</p><figure align="center"><img src="/2018/07/19/face-alignment-cnn/2.png" width="60%"></figure><p>[参考文献]:<br>[1] <a href="https://arxiv.org/pdf/1706.01789.pdf" target="_blank" rel="noopener">《Deep Alignment Network: A convolutional neural network for robust face alignment》</a><br>[2] github: <a href="https://github.com/MarekKowalski/DeepAlignmentNetwork" target="_blank" rel="noopener">MarekKowalski/DeepAlignmentNetwork</a>  </p>]]></content>
    
    <summary type="html">
    
      &lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;/2018/07/19/face-alignment-cnn/1.png&quot;&gt;
&lt;/figure&gt;

&lt;p&gt;这部分是去年 9 月份开始的工作，算是第一次真正踏入深度学习的领域。具体工作也还算简单，就是复现一篇深度学习方法做的人脸对齐，当练练手。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术经验" scheme="https://lamply.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BB%8F%E9%AA%8C/"/>
    
      <category term="人脸对齐" scheme="https://lamply.github.io/tags/%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/"/>
    
  </entry>
  
  <entry>
    <title>传统方法的人脸对齐</title>
    <link href="https://lamply.github.io/2018/07/15/face-alignment/"/>
    <id>https://lamply.github.io/2018/07/15/face-alignment/</id>
    <published>2018-07-15T04:11:12.000Z</published>
    <updated>2019-03-05T15:02:43.348Z</updated>
    
    <content type="html"><![CDATA[<p><div align="center"><img src="/2018/07/15/face-alignment/1.png">  </div><br>这里是关于应用传统方法做人脸对齐的经验总结，是在去年5月到7月的工作，也是我入职后的第一个正式项目，用的是 SDM (Supervised Descent Method) [1] 的方法，具体细节可能不太记得，所以会慢慢补完。<br><a id="more"></a></p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>在深度学习杀到这领域前，有两种主流的人脸对齐方法，一个是14年的号称能达到 3000FPS 的 LBF，还有一种就是13年的 SDM。由于 3000fps 复现效果不理想，实际上 SDM 比起 3000fps 精度要高一些，而且还有不错的现成代码，当时也只是想把静态的人脸对齐做了，所以就选用了 SDM。 </p><p>关于 SDM，其实是作者提出的一种非线性最小二乘优化的方法，类似牛顿步等方法，只不过回避了需要大计算量的 Hessian 矩阵和 Jacobian 矩阵的计算，人脸对齐算是它的一种应用。论文中的大致意思就是从牛顿步出发想办法把那两矩阵裹起来改为用迭代回归来学习，得到下降方向和大小，关于它更多的理论理解部分在官网上有很简单直观的介绍，这里暂时先放着。  </p><h3 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h3><p>我用的是 github 上 patrikhuber[2] 的开源代码，这个版本比论文的 SDM 稍微改进了一些，不过当然无论是模型大小、运算速度还是算法效果，都远不足以应用。所以我花了不少时间看论文，并在源代码基础上实现了一大堆算法模块，然后不停训练。当然很多想法实际上并没有奏效。 </p><p>那段时间现在看起来是挺盲目的，想到什么加什么，甚至还纠结用 HOG 还是 DSIFT 做特征提取，还是两者混合 SDM 迭代时切换特征提取器。在诸如此类想法上花了不少工程上的功夫以达到能够随时切换配置做训练，更重要的是浪费了训练时间。实际上这些都不是制约最终效果的瓶颈，因为当时的特征提取的范围是限定的，瓶颈并不在于特征提取器本身，况且 HOG 和 DSIFT 本身差别并不大，在瓶颈时如果不找出瓶颈下功夫而着眼于其他不确定的想法往往是不明智的。  </p><p>其实那时候很多的想法可能并不是没有奏效，而是被瓶颈掩盖了，造成了想法无效的错觉。换句话说，我们做算法的常常要评判什么想法是有效的，什么想法是无效的，而我觉得这种评判是要有所保留的，可能之所以想法无效是因为被什么我们没看到的因素所制约了，暂时行不通罢了。  </p><p>扯远了，总之当时的瞎蒙乱撞最终幸运的还是找到了几个有用的改进方法。类似《Extended Supervised Descent Method for Robust Face Alignment》[3] 里提到的，一个是关于特征提取范围以及cell数量的改进，改成了顺应 SDM 迭代过程从大到小的范围、从粗到细的计算；第二个就是分开全局和局部来进行回归。这一些改进其实算是人脸对齐中比较常见的改进了，没有太多新意，不过对于全局和局部区分回归来说，比起对最终效果的改进，这个方法对模型大小和运算速度的改进更为明显。最后为了工程上的应用，这些方法都需要大量调参。  </p><p>完成上面的这些其实也提升不了太多，关于这个项目最终提升最大的还是加入了人脸检测时得到的 5 个点做先验。也就是把 5 点扩展到 68 点，再加点 tricks 使这 68 点极其接近 ground truth，最后作为 init shape 输入到 SDM。这个方法让人脸对齐准确率和成功率大大增加，因为先验降低了回归的难度，把瓶颈推到了人脸检测时的五点回归成功率。虽然某些特殊情况可能会因为先验产生一些误导，不过也让通常情况下的对齐准确度达到非常高的地步，这是值得的。（后来我记得找到了一篇2017年7月的论文也有提到用类似我这个方法的改进版 SDM 和 LBF 来和他的深度学习方法对比的论文，只不过他用的是 JDA，我用的是 MTCNN，还有就是生成的 init shape 方法略有不同。）  </p><h3 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h3><p>至此，我的 SDM 人脸对齐的效果已经和当时竞品水平相当了，最终在 300w 上测试的结果为（用的 inter-pupil normalization error）  </p><div class="table-container"><table><thead><tr><th style="text-align:center">Full set (%)</th><th style="text-align:center">Common subset (%)</th><th style="text-align:center">Challenging subset (%)</th></tr></thead><tbody><tr><td style="text-align:center">4.75</td><td style="text-align:center">4.09</td><td style="text-align:center">7.47</td></tr></tbody></table></div><p>当然了，因为这个测试数字是用了五点 ground truth 做抖动的前提下得到的，所以没严格的参考价值，无法和其他公开方法做比较，不过也能反映这种方法的有效性，因为除了一些较为困难的情况，大多数时候通过 MTCNN 得到的五点都不会有太大偏差。   </p><p>最终做了三个模型，分别是有五点先验的 68 点、106 点，以及没有五点先验的 106 点。模型大小都是 5MB 左右， 在小米 mix2 上速度 20ms 左右。demo 效果大致如下：</p><figure align="center"><img src="/2018/07/15/face-alignment/2.png" width="80%"></figure>  <h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>关于训练数据集其实还有非常多要写，比如我这里只用了 300-w 数据集，在这基础上翻转、模糊、调对比度，还有关于如何将 68 点数据集扩增到 106 点的，也参考复现了几篇论文。还有的话就是安卓端的部署，包括算法的移植、接口封装，运算速度的优化，模型的压缩等等。之后有空的话可能会整理补上。</p><p>[参考文献]:<br>[1] <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Xiong_Supervised_Descent_Method_2013_CVPR_paper.pdf" target="_blank" rel="noopener">《Supervised Descent Method and its Applications to Face Alignment》</a><br>[2] github: <a href="https://github.com/patrikhuber/superviseddescent" target="_blank" rel="noopener">patrikhuber/superviseddescent</a><br>[3] <a href="http://pdfs.semanticscholar.org/5c82/0e47981d21c9dddde8d2f8020146e600368f.pdf" target="_blank" rel="noopener">《Extended Supervised Descent Method for Robust Face Alignment》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/2018/07/15/face-alignment/1.png&quot;&gt;  
&lt;/div&gt;&lt;br&gt;这里是关于应用传统方法做人脸对齐的经验总结，是在去年5月到7月的工作，也是我入职后的第一个正式项目，用的是 SDM (Supervised Descent Method) [1] 的方法，具体细节可能不太记得，所以会慢慢补完。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术经验" scheme="https://lamply.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BB%8F%E9%AA%8C/"/>
    
      <category term="人脸对齐" scheme="https://lamply.github.io/tags/%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/"/>
    
  </entry>
  
  <entry>
    <title>《哥德尔·艾舍尔·巴赫——集异璧之大成》其四</title>
    <link href="https://lamply.github.io/2018/07/11/%E4%B8%80%E8%87%B4%E6%80%A7%E5%AE%8C%E5%85%A8%E6%80%A7%E4%B8%8E%E5%87%A0%E4%BD%95%E5%AD%A6/"/>
    <id>https://lamply.github.io/2018/07/11/一致性完全性与几何学/</id>
    <published>2018-07-11T08:14:00.000Z</published>
    <updated>2018-07-25T07:40:38.910Z</updated>
    
    <content type="html"><![CDATA[<p>本文是 GEB 的第四章部分阅读笔记，这章开始深入对形式系统的意义起效做讨论，介绍了一致性、完全性，还有用于阐述未定义项的几何发展史。<br><a id="more"></a></p><h3 id="引论"><a href="#引论" class="headerlink" title="引论"></a>引论</h3><p>前面部分看着挺容易理解，不过不太好总结，主要是通过对位藏头诗来讲述意义的层次，即显明的意义和隐含的意义。而各层次的意义通过同构来传达。最后引出哥德尔定理，这是对位藏头诗的主要目的。  </p><p>这个对位藏头诗例子不太亲切，主要是关于唱机的唱针沿唱片纹道颤出声音，外界的所有声音和唱机本身也震颤，那总有一种唱片可以让唱机因为震颤而自毁。除此之外还有非常多隐藏的同构，就不列举了。  </p><p>接着介绍了不完全性，即真理超出了形式系统的定理资格的规定。然后深入形式系统讨论 真理 和 定理资格。  </p><p>这里举了个例子，把 pq 系统稍改一下，加入一条公理模式：若 x 是一个短杠串，则 xqxp- 是一个公理，那么之前的pq系统的解释就不能完全应用了，况且新旧定理存在不一致，比如 “—q-p-“ 和 “-q-p-“ 都是公理。为了重新获得一致性，我们只要把 q 的解释改为小于或等于就可以了。  </p><h3 id="几何发展史"><a href="#几何发展史" class="headerlink" title="几何发展史"></a>几何发展史</h3><p>通过刚才修改pq系统的例子，作者引出了对几何发展的历史的探讨。  </p><p>首先是欧几里得通过《几何原理》将当时的几何知识从最简单的概念和定义开始逐渐地、精细地建立成庞大的几何学体系。而所有的这些都只基于五条公设：  </p><ol><li>一条直线段可以联接两个点。</li><li>一条直线上任何一条直线段可以无限延伸。</li><li>给定一条直线段，可以以一个端点为圆心，以此线段为半径做一个圆。</li><li>一切直角都彼此相等。</li><li>如果两条直线与第三条直线相交时，在第三条直线的某一侧三条线所夹的内角之和小于两个直角的和，则那两条直线沿着这一侧延伸足够长之后必然相交。</li></ol><p>可以看到第五条公设不那么优雅，只是没有办法证明，所以只能设定为公设。（现在称没有第五条公设帮助的几何部分为绝对几何，额外满足第五公设的就是欧式几何，不满足第五公设而满足另外一个公设的就是非欧几何）  </p><p>后继者们为了证明第五公设花了很多力气，不过都是错的。其中有意思的是济罗拉莫·萨彻利曾通过反证得出一个“与直线的本质相抵触”的命题，不过他找到这个矛盾过后就不干了，这让他与后来“双曲几何学”失之交臂。而五十年过后，兰伯特也重复了这种失之交臂。直到再过了四十年，非欧几何被认可了。  </p><p>1823年非欧几何同时被J.鲍耶和罗巴切夫斯基发现，有意思的是同一年法国数学大家勒让德在相当程度上沿着萨彻利的路子想出了第五公设的一个证明。尽管其他人研究了很久一直都没有结果，但非欧几何的出现在那个时候是有奇妙的同时性的。这种科学发现的同时性是挺有趣的一个现象。  </p><h3 id="一致性、完全性"><a href="#一致性、完全性" class="headerlink" title="一致性、完全性"></a>一致性、完全性</h3><p>非欧几何的发现带来了一种启示，就是未定义项的问题。大体来说，得到“与直线的本质相抵触”的结果，是因为没有摆脱先入为主的“直线”概念。如果把它们当成未定义项，仅由公设来给它们定义，那就能得到不同的结果。这就跟之前 pq 系统中我们修改了 q 的解释一样，摆脱了先入为主的观念，从满足新的定理上重新找到了 q 的正确解释。  </p><p>（待续）</p><p><br></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是 GEB 的第四章部分阅读笔记，这章开始深入对形式系统的意义起效做讨论，介绍了一致性、完全性，还有用于阐述未定义项的几何发展史。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书笔记" scheme="https://lamply.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《哥德尔·艾舍尔·巴赫——集异璧之大成》其三</title>
    <link href="https://lamply.github.io/2018/06/28/%E5%9B%BE%E5%BD%A2%E4%B8%8E%E8%A1%AC%E5%BA%95/"/>
    <id>https://lamply.github.io/2018/06/28/图形与衬底/</id>
    <published>2018-06-27T16:00:00.000Z</published>
    <updated>2018-07-11T06:05:18.663Z</updated>
    
    <content type="html"><![CDATA[<p>本文是 GEB 的第三章部分阅读笔记，内容多是我自己筛选、压缩、重排的，并尽可能的保留书中名词的译名和说法。因为我只有非数学系普通理工科大学生的数学基础，不怎么熟悉数论逻辑学什么的，所以可能会有某些错误的理解或说法，就酱。<br><a id="more"></a></p><h3 id="素数系统"><a href="#素数系统" class="headerlink" title="素数系统"></a>素数系统</h3><p>这部分先是引入描述素数的形式系统，然后试图通过构建其他更简单的类似形式系统来达到目的。<br>比如书中使用了一个看上去很聪明的方法：  </p><ol><li>构建tq系统（类似pq系统的乘法版本）</li><li>制定Cx定理（如果 xqy-tz- 是定理，则Cx是定理）来刻划合数（因为y- z- 为数量大于一的短杠）</li><li>草拟一个规则（如果Cx不是一个定理，则Px是个定理）来刻划素数  </li></ol><p>这样看上去似乎就完成了，然而这里面有一个重大缺陷：Cx是否是一个定理不是能够明显表达的印符操作，就像 WJU 系统中 WU 是不是定理没办法一下子看出来一样，我们必须要到形式系统之外才能推出什么什么不可能出现在定理中， 这是不符合被允许的印符操作的。</p><p>这里有个很关键的事情，就是当我们以 惟方式（W） 来工作时，我们很经常会混淆符号串与符号串的解释，把 —- 与 3 做同等对待，从而得出一些错误的结论。之前对形式化的要求就显得额外重要，我们不能把算术事实与印符定理相混淆。  </p><p>从这里书中开始谈论非定理的形式问题，我们可以知道定理都是具有共同“形式”的，即Cx，当x为合数数量的短杠时Cx为定理，那对应的非定理是否也是有同一种“形式”呢？答案是对，也不对（这里书中排除了非定理中非良构的符号串，比如 tt-Cqq 这种乱七八糟的东西），无可否认它们都具有某种印符特性，但这是否能称为“形式”确是不清楚的，因为它们是以否定的方式定义的。  </p><p><strong><strong>（题外话：可能会有人跳出来说，Cx规则与合数同构，因为合数反过来就是素数，所以Cx的良构非定理就和素数同构啦。虽然书中并没有对这个做论证，但我想大概就像之前说得那样，这种惟方式是不能用于替换推导的，后面可能就会有完全符合形式系统的方式来得出这种结论。）</strong></strong><br><br></p><h3 id="图形与衬底"><a href="#图形与衬底" class="headerlink" title="图形与衬底"></a>图形与衬底</h3><p><em>到这里书中跳到了另一个层次，讲述刚才的否定在其他层面的形象体现，并通过类比（Analogy，传说中 GEB 的核心）把这种否定观念代回到形式系统中。到这里就能明白之前阿基里斯和乌龟电话里的意思了，虽然我早就看出谜底是蜡烛，不过现在看来乌龟的图形与衬底的提示其实就是映射了“虫虫”谜题和“昔火”谜题，说明这章的主旨正是这种正反相衬。</em><br><br><br>这部分主要介绍了图形和衬底，一般来说，我们看到一幅画，会关注它的图形，也可以叫前景、正空间，不可避免的会产生衬底，也叫背景、负空间。那么除了一般的只有图形有意义，衬底只是附带的，那这幅画书中称作流畅可画出；如果图形和衬底都有意义，就叫倍流畅，就是双倍流畅的意思。埃舍尔就很擅长倍流畅的画，整幅画没有附带的部分，各个区域都是有意义的画像。  </p><p>然后在音乐中也存在这种现象，对应的是旋律与伴奏，还有音符落在强半拍和弱半拍的交织。  </p><p>回到形式系统，我们的目标是把作为负空间表示的素数个短杠符号串Cx改成用正空间表示的Px。从图形与衬底包含相同的信息来看，似乎这是一般意义上可行的，但是事实上只是在这个问题上才可行，有这样一个事实：  </p><p>  存在一个形式系统，其负空间（非定理集）不是任何一个形式系统的正空间（定理集）。  </p><p>换个更专业的描述即是，存在非递归的递归可枚举集。这里“递归可枚举”（缩写r.e.）对应艺术上的流畅可画出，简而言之就是可以按照印符规则生成的集合（所有形式的系统定理集）；“递归”则是对应倍流畅，意思是不仅它本身是r.e.，其补集也是r.e.。  </p><p>这就导致了存在一些形式系统，它们没有用印符规则表述的判断过程。  </p><p>关于这个论点的证明，作者没有详细给出，只是把上面的事实“当作信念而接受”。后面以所有图形都是倍流畅的吗来类比所有的集合都是递归的吗，然后完结这一个议题。  </p><p>最后部分给出了生成素数的形式系统，使用不整除和没有因子的规则来形成单向测试，这里不详细记录了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是 GEB 的第三章部分阅读笔记，内容多是我自己筛选、压缩、重排的，并尽可能的保留书中名词的译名和说法。因为我只有非数学系普通理工科大学生的数学基础，不怎么熟悉数论逻辑学什么的，所以可能会有某些错误的理解或说法，就酱。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书笔记" scheme="https://lamply.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《哥德尔·艾舍尔·巴赫——集异璧之大成》其二</title>
    <link href="https://lamply.github.io/2018/06/27/%E5%85%B3%E4%BA%8E%E5%BD%A2%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    <id>https://lamply.github.io/2018/06/27/关于形式系统/</id>
    <published>2018-06-26T16:00:00.000Z</published>
    <updated>2018-07-11T05:58:28.587Z</updated>
    
    <content type="html"><![CDATA[<p>本文是「GEB」第二、三章的读书笔记，这部分是边读边记，同时忽略了一些详细严谨的解释和讨论。<br><a id="more"></a></p><h4 id="pq系统"><a href="#pq系统" class="headerlink" title="pq系统"></a>pq系统</h4><p>首先引入一个简单构造的系统，一个由「p、q、-」三个符合组成的系统，它具有无数条公理，通过公理模式「x为一组短杠”-“，那么”x-qxp-“为一条公理」来产生。其生成规则只有一条「x、y、z为只包含短杠的符号串，若xqypz为定理，则x-qypz-为定理」。  </p><p>因为只包含了加长的规则，所以可以说这个系统存在判定过程。可以找出其判定过程就是：对于任意一个xqypz符号串，x、y、z为仅由短杠组成的符号串，若其数量x=y+z，则xqypz是一个定理。这属于一种自顶向下的判定过程。  </p><p>到这里就涉及到了一个书中的中心问题。可以看到pq的定理和加法的相似，比如 ——-q—p—- 是一条定理，因为5=2+3。但仔细一想，「——-q—p—-是一条定理」是否就和「5=2+3」是一样的呢，我们之所以会这么想，是因为我们在pq定理和加法运算之间看到了「同构」，即两个复杂结构可以互相映射，两者各部分都有两两对应，比如 ——- 对应 5，q 对应 =，— 对应 2，p 对应 +，—- 对应 3。这种对应关系有一个名称：解释。</p><p>其次，这种对应也存在于经过解释的定理和真陈述（真理）之间，比如刚才的pq定理，尽管它可以用其他猪牛马做解释，但唯有用加法来解释时同构存在于定理和现实的某部分中，所以这种解释是有意义的。  </p><p>值得注意的是，pq定理的加法解释是只有在其符合pq形式系统的形式（即xqypz形式）时才能够与真理同构的，这种形式上的符合称为「良构」。若符号串不是良构的，那就不能为此符号串赋予同等意义的解释，比如 ————q—p—p—p—，尽管 8=2+2+2+2，但该符号串并不是良构的，所以这不是一条定理，更不能赋予其意义。在这个基础上可以看到，形式系统的意义一定是被动的。  </p><p>后面的部分大致是关于将形式系统推广的时候遇到的障碍。要构造一个和真理完全同构的形式系统时，必须要让每一个定理为真理，每一个非定理为“假理”。但当定理集合无穷的情况下，我们怎么知道所有的定理都在这种解释方法下表达了真理？这里书中介绍了抽象理想的数，并引入了“欧几里得定理”的证明过程，这种证明的陈述具有紧密的上下联系，使得大家最终都必须相信看上去并不显然的结论。这个证明过程使用了新的特定的词汇，用于抽象描述数的性质，避开了无穷。  </p><p><br><br>到这里第二章结束。这一部分算是初步介绍了形式系统，从前面的pq系统开始介绍，后面似乎开始泛化，有点难抓住主题，每一小段都是单独一方面的具体描述，我有点云里雾里，只是隐约感受到开始将形式系统完备到能应用的有意义的状态，那些描述趋向于说明一些困难，以及某一些解决的手段。我觉得这里的困难大概就是前面乌龟和阿基里斯的对话那里隐喻的问题，非常地逻辑学和数学……  </p><p>后面几章会构造一个形式系统，并似乎要讨论关于该系统能否理论上达到我们的思维能力的水平。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是「GEB」第二、三章的读书笔记，这部分是边读边记，同时忽略了一些详细严谨的解释和讨论。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书笔记" scheme="https://lamply.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《哥德尔·艾舍尔·巴赫——集异璧之大成》其一</title>
    <link href="https://lamply.github.io/2018/06/21/WU%E8%B0%9C%E9%A2%98/"/>
    <id>https://lamply.github.io/2018/06/21/WU谜题/</id>
    <published>2018-06-20T16:00:00.000Z</published>
    <updated>2019-03-05T14:50:34.642Z</updated>
    
    <content type="html"><![CDATA[<p>学习之余看起了以前没看完的书，顺便把博客环境重新搭了起来。<br><a id="more"></a></p><h3 id="WJU-谜题"><a href="#WJU-谜题" class="headerlink" title="WJU 谜题"></a>WJU 谜题</h3><p>这是在「GEB」里出现的一个谜题，原版书中是MU谜题，不过意思一样，讲的是通过在一个只包含WJU三个字符串的形式系统内给出WJ字符串要通过四条规则来产生WU字符串的谜题，这四条规则是：  </p><ol><li>如果字符串以J结尾则可以在其后加上一个U，即WUJ -&gt; WUJU</li><li>字符串  Wx 可以扩展为 Wxx，其中「x」为W后面的所有字符串，比如 WJ -&gt; WJJ, WUJ -&gt; WUJUJ</li><li>连续的三个J可以替换成一个U，即 JJJ -&gt; U，但一个U不能替换成三个J</li><li>连续的两个U可以剔除  </li></ol><p>整个系统产生的字符串都是顺序相关的，WJU与WUJ是不同的字符串。那么给定WJ字符串能否转换成WU呢？  </p><p>说实话，这章看得我一愣一愣的，因为这个谜题实际上是不可解的，J只能通过倍增以及三倍加减来变换，即含有质数2，这样得出的J的数量是不可能为3所整除的（假设U与3倍J等价），而书中以此谜题为例子后面却没有对这个谜题做出解答，转而阐述人类智能在处理形式系统时和机器的差异以及其他东西。我觉得如果能稍微提示下花个十来分钟做尝试，然后最后给出答案，再进行深入的探讨会合适很多。而且后面关于判定过程的部分翻译也有些错误和含糊，看得一愣一愣的就突然结束了话题。  </p><p>只能说这本书的写作风格不太符合一般的读书方式，好在大部分的意味都能理解到。</p><h5 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h5><p>提出该WJU系统的意义在于阐述对于一个形式系统的三种处理方式：书中称之为惟方式（W），机方式（J），无方式（U）。也就是分别对应从规则入手，在形式系统外审视其规则做出总结归纳，以及完全按照形式系统的规则像机器一样做运算。</p><p>[参考资料]<br>wiki： <a href="https://en.m.wikipedia.org/wiki/MU_puzzle" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/MU_puzzle</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习之余看起了以前没看完的书，顺便把博客环境重新搭了起来。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="读书笔记" scheme="https://lamply.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>零核现象</title>
    <link href="https://lamply.github.io/2018/04/19/zero-kernels/"/>
    <id>https://lamply.github.io/2018/04/19/zero-kernels/</id>
    <published>2018-04-19T11:17:12.000Z</published>
    <updated>2019-03-05T14:58:47.734Z</updated>
    
    <content type="html"><![CDATA[<p><div align="center"><img src="/2018/04/19/zero-kernels/TingMengDe.bmp">  </div><br>这里是对零核现象的观察实验记录.<br>具体来说, 就是在训练卷积神经网络的过程中发现模型中有大量卷积核的 L1 变为 0 的情况, 这里为了方便简称零核现象.<br><a id="more"></a>  </p><p>最初遇到这种问题是在 <a href="https://github.com/MarekKowalski/DeepAlignmentNetwork" target="_blank" rel="noopener">DAN</a> 训练时发现的, 当时觉得是太大学习率, ReLU 死亡, 后面降低了 lr 就没出现过了.<br>直到后来做分割观察 <a href="https://github.com/TimoSaemann/ENet" target="_blank" rel="noopener">ENet</a> 的预训练模型时又发现了几百个零核的现象, 而且自己用 ShuffleNet 做的分割网络也出现了非常多零核, 这对模型性能显然是有很大影响的, 所以就下定决心解决这个问题.<br>根据之前 DAN 的经验, 我自然先试了一下降低学习率, 结果没用, 虽然零增长的速度变慢了, 但还是会出现, 而且随着训练过程零核几乎线性增加 ( 像上图那样 ).<br>把每层的零核数作纵坐标, 层数作横坐标, 打印成曲线出来就是这个样子:  </p><p><div align="center"><img src="/2018/04/19/zero-kernels/coco_train.png">  一共 2312 个零核, 简直壮观.  </div><br><br></p><p>因为零核多集中在 depthwise 卷积上, 所以感觉上可能是由于 depthwise 卷积核太薄, 容易训练时掉坑回不来. 后面在网上也没找到多少关于这个的讨论, 唯一一个是在知乎上 <a href="https://www.zhihu.com/question/265709710/answer/298245276" target="_blank" rel="noopener">关于 MobileNet V2 的回答</a>, 也是差不多的解释, 不过我后来去掉了后面所有的 ReLU, 也是得到了很多空核, 也是个迷.<br>无奈之下开始各种调超参. 一是把 batch size 加大, 讲道理更新得会稳一些, 然而并没有用, 零核依然会出现. 二是换了优化器, 用回朴素的 SGD momentum. 这时神奇的事情发生了, 不管怎么训练, 怎么调大 lr, 调小 batch size, 零核都没有出现了…</p><p><div align="center"><img src="/2018/04/19/zero-kernels/adam.png">  Adam<img src="/2018/04/19/zero-kernels/sgd.png">  SGD</div><br>最终对比了几个数据集, 从结果上来看 SGD 版比 Adam 版泛化性更强, 性能在个别数据集上也提升很大, 测试指标的标准差更是明显低于 Adam 版的, 做分割出来的边界也变得更加平滑了. 毕竟 Adam 版零核集聚在高层次上, 泛化方面有所缺陷的也是正常的.  </p><h3 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h3><p>后面有空在 mnist 上做了些实验, 发现优化器中只有 Adam 和 RMSProp 肯定会产生零核, 而用其他 SGD, AdaDelta, AdaGrad 都不会产生零核. 既然如此的话, 似乎是可以从 RMSProp 中找到启示的, 但如果要验证还是得具体分析下更新过程才行, 只能暂时留坑了. </p><h3 id="接续"><a href="#接续" class="headerlink" title="接续"></a>接续</h3><p>好吧, 上一次记录的结果是错误的, 并非只有 Adam 和 RMSProp 肯定会产生零核, 理论上零核的产生依旧是和参数更新的速率密不可分, 所以所有优化方法都可能产生零核. 之所以之前产生错误的结论, 是因为统计零核时采用了 L1 + 阈值 的方法, 而实际上零核表现出来的是并未完全收敛于 0, L1 差均值 10 倍以内, 但相较于其他滤波器而言判别力非常低的情况. 比如下面这种.<br>下面是用 AdaGrad 优化一个普通卷积接 depthwise 卷积重复三次的简单网络, 数据集用的 fashion-mnist, 图为其中两层相邻卷积的可视化, 红橙黄绿蓝靛紫, 代表卷积核的绝对值大小, 左边为普通卷积沿通道绝对值叠加得来, 右边为 depthwise 卷积取绝对值得来.  </p><p><div align="center"><img src="/2018/04/19/zero-kernels/bad.png"></div><br>其后两层</p><p><div align="center"><img src="/2018/04/19/zero-kernels/bad2.png"></div><br>可以看到, 部分卷积核已经几乎一片红了, 对其上层卷积核也产生了影响. 相比之下, SGD 训练的好的情况:</p><p><div align="center"><img src="/2018/04/19/zero-kernels/good.png"></div><br>虽然还无法解明什么, 但至少说明了 depthwise 卷积不太好训练, 通过观察训练过程卷积核的变化, 可以看到 SGD + momentum 相对还是比较平稳的, 尽管有些时候可能也会漏网 ( 实际上之前用 MobileNetV2 做分割在 COCO 上预训练也有出现十几个零核… ). 先到这里, 之后一年的时间因为要专心学习, 所以大概要全面搁置了, 可能会整理记录下之前的项目. 就等之后爬上好的平台再说吧.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/2018/04/19/zero-kernels/TingMengDe.bmp&quot;&gt;  
&lt;/div&gt;&lt;br&gt;这里是对零核现象的观察实验记录.&lt;br&gt;具体来说, 就是在训练卷积神经网络的过程中发现模型中有大量卷积核的 L1 变为 0 的情况, 这里为了方便简称零核现象.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="观测" scheme="https://lamply.github.io/tags/%E8%A7%82%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>MobileNet V2</title>
    <link href="https://lamply.github.io/2018/03/01/MobileNetsV2/"/>
    <id>https://lamply.github.io/2018/03/01/MobileNetsV2/</id>
    <published>2018-03-01T13:12:06.000Z</published>
    <updated>2019-10-11T16:29:30.852Z</updated>
    
    <content type="html"><![CDATA[<p>这是关于轻量级网络 MobileNet 的改进版论文，作为万众瞩目的高效率骨干网络架构，它的更新意味着移动端网络的又一次改进。<br>原文链接： <a href="https://arxiv.org/pdf/1801.04381.pdf" target="_blank" rel="noopener">Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation</a><br><a id="more"></a></p><h3 id="Main-Contribution"><a href="#Main-Contribution" class="headerlink" title="Main Contribution"></a>Main Contribution</h3><p>inverted residual with linear bottleneck. 输入低维压缩表征, 扩增到高维并进行 depthwise 卷积, 再通过线性卷积投射回低维表征.<br>该卷积结构因为不需要完全实现大的中间 feature map, 还能显著降低内存占用  </p><h3 id="Pre-Knowledge"><a href="#Pre-Knowledge" class="headerlink" title="Pre-Knowledge"></a>Pre-Knowledge</h3><ol><li><p>Depthwise Separable Convolutions<br>广泛应用的标准卷积替代品. 将标准卷积分解为两部分, 第一部分为 <em>depthwise convolution</em>, 即每一个输入通道使用对应的一个卷积核来滤波, 第二部分为 <em>pointwise convolution</em>, 即使用 1x1 卷积将上层特征线性组合得出新的特征.<br>\(d_o\) 个 \(k \cdot k\) 标准卷积花费 \(h_i\cdot w_i\cdot d_i\cdot d_o\cdot k^2\), 而相对应的 depthwise 分离卷积花费 \(h_i\cdot w_i\cdot d_i\cdot k^2 + h_i\cdot w_i\cdot d_i\cdot d_o\), 相当于减少了近 \(k^2\) 倍的计算量 ( 实际是乘了 \(\frac{1}{d_o}+\frac{1}{k^2}\) 倍 )</p></li><li><p>Linear Bottlenecks<br>这里讨论激活层的基本属性, 文中将激活层的 feature map 看作维度 \(h_i\times w_i\times d_i\) 的激活张量.<br>正式来说, 对于 \(L_i\) 层, 输入一组图像, 其激活组成了一个 “ manifold of interest “ ( 感兴趣流形? )<br>这里提出了关于 manifold of interest 的一些假设, 并通过实验证明在 bottleneck 中使用非线性会损坏信息, 给出了通过在卷积 block 内插入 linear bottleneck 可以达到 capture 低维 manifold of interest 的目的  </p></li><li><p>Inverted residuals<br>受 bottlenecks 实际上包含所有必要信息的直觉启发, 文中直接将 bottlenecks 间作为 shortcuts<br>最终该设计网络的层可以去除而不需要重新训练, 只减少一点点准确率</p></li><li><p>信息流的解释<br>文中阐述了该结构能将 building blocks 的输入和输出域很自然的分割开来, 作为网络每层的容量, 以及输入输出间的非线性函数作为表达力 ?  </p></li></ol><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>Bottleneck &amp; Network:  </p><p><div align="center"><img src="/2018/03/01/MobileNetsV2/bottleneck.png"><img src="/2018/03/01/MobileNetsV2/block.png"><img src="/2018/03/01/MobileNetsV2/network.png"></div><br>在作者实验中, t 在 5~10 之间得出的结果大部分差不多, 只是在小网络中小的 t 值会稍微好一点, 大的网络中大的 t 值会稍微好一点<br>在整体网络结构中, 第一个 bottleneck t 值为 1 会比较好, 另外在小的网络宽度下, 保留最后几层卷积层的卷积核数会提高小网络的性能  </p><h3 id="Implementation-Notes"><a href="#Implementation-Notes" class="headerlink" title="Implementation Notes"></a>Implementation Notes</h3><p>这部分主要是关于推理时内存占用的优化问题, 大致来讲, 传统的结构的内存占用由并行结构主导 ( 即残差连接之类的 ), 而这类结构需要内存为通过计算的输入和输出 tensor 的总和<br>而文中提出的 building block 的内部操作均为 per-channel 的, 且随后的非 per-channel 操作具有很大的输入输出 size 比率 ( 即 bottleneck 的输入 channel 显著大于输出时的 channel ). 这样在内部操作中需要的内存占用仅仅为一个 channel 的 size, 而输出残差连接也因为 Invert Residual 减少了 channel 从而减少内存占用  </p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>比较关心分割方面的, 所以这里只记录关于分割的实验<br>实验在 PASCAL VOC 2012 上进行, 主要是比较各种 feature extractors 下的 DeepLab V3, 以及简化 DeepLab V3 的方法, 推理时采取不同的策略来提升性能. 结论如下  </p><ol><li>推理策略加入 multi-scale 和 left-right flipped 会显著地增加计算量, 所以在终端设备应用上不考虑</li><li>output_stride 为 16 比 8 更高效</li><li>在倒数第二层基础上做 DeepLab V3 会更高效, 因为 channel 小, 而且得到的性能相似</li><li>去掉 ASPP 会减小很多计算量而且只损失一点性能 <strong><em>( ASPP 没有进行 depth-wise 改进, 参见 DeepLab V3, 所以该结论在优化后的 ASPP 上实际效果存疑 )</em></strong></li></ol><h3 id="Ablation-study"><a href="#Ablation-study" class="headerlink" title="Ablation study"></a>Ablation study</h3><p>两个方面</p><ol><li>Invert residual connections 的有效性</li><li>违反常理的 linear bottleneck 能提升性能, 给 non-linearity 操作在 bottleneck 低维空间内损失信息的假设提供了支持</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是关于轻量级网络 MobileNet 的改进版论文，作为万众瞩目的高效率骨干网络架构，它的更新意味着移动端网络的又一次改进。&lt;br&gt;原文链接： &lt;a href=&quot;https://arxiv.org/pdf/1801.04381.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文笔记" scheme="https://lamply.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>ShuffleNet</title>
    <link href="https://lamply.github.io/2018/03/01/ShuffleNet/"/>
    <id>https://lamply.github.io/2018/03/01/ShuffleNet/</id>
    <published>2018-03-01T13:12:06.000Z</published>
    <updated>2019-10-11T16:49:09.633Z</updated>
    
    <content type="html"><![CDATA[<p>这部分是关于轻量级网络 ShuffleNet 的论文记录，主要是基于 channel shuffle 的想法来减少 CNN 中占大头的 1x1 卷积的计算量。<br><a id="more"></a></p><h3 id="ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices"><a href="#ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices" class="headerlink" title="ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"></a>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</h3><hr><h4 id="原文理解"><a href="#原文理解" class="headerlink" title="原文理解"></a>原文理解</h4><p>介绍一种极其计算高效的 CNN 结构 ShuffleNet, 计算量可以控制在 10-150M FLOPs, 利用了 1x1 group conv, depth-wise conv 和 shuffle channel op.<br>比起现有的方法专注于对基础网络进行 pruning, compressing, low-bit representing, 本文追求在非常有限的计算资源下 ( 几十或几百 MFLOPs ) 达到最好的效果. 目标是探索针对所需计算量范围定制非常高效的基础网络.<br>ResNext 和 Xception 由于 1x1 dense 卷积过多在小网络中效率不高, 本文提出 <em>pointwise group convolution</em> 减少 1x1 卷积的计算复杂度, 同时经过 <em>channel shuffle</em> 来克服边缘效应</p><h5 id="channel-shuffle"><a href="#channel-shuffle" class="headerlink" title="channel shuffle"></a>channel shuffle</h5><p>在小网络下 1x1 卷积数量大很昂贵, 减少通道可能会对精度产生很大的损害. 最直观的解决方法是采用 channel sparse connections, 比如 group conv. 但这又导致输出部分仅与其相应输入的部分通道产生, 导致信息流通阻塞.<br>通过 <em>channel shuffle</em> : 输出 g x n 通道, 对输入先 reshape 到 (g, n), 转置然后 flatten. 可以优雅地解决问题.</p><h5 id="ShuffleNet-Unit"><a href="#ShuffleNet-Unit" class="headerlink" title="ShuffleNet Unit"></a>ShuffleNet Unit</h5><p>基于 bottleneck 的结构 (b) (c): </p><p><div align="center"><img src="/2018/03/01/ShuffleNet/Unit.png">  </div><br>为了简单, channel shuffle 只加在第一个 1x1 后. 由于在低功耗设备下 depth-wise conv 的 <code>computation/memory  access  ratio</code> 可能比 dense op 要差, 实际上并不能高效实施, 所以故意只在 bottleneck 中使用.</p><h5 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h5><p>参考设计: 类似 ResNet, bottleneck channel 是 output channel 的 1/4, 下层 channel 翻一倍.<br>在 ShuffleNet 中, 大致相同计算量下, 明显可以看出, group 数越大, output channel 就要越多, 这有助于 encode 更多信息, 尽管有可能会导致单个卷积滤波器的作用降级<br>另外, 在第一次 point-wise conv 时不采用 group conv, 因为输入通道数相对较小</p><h5 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h5><p>和 MobileNet 一样, 这里训练使用了不那么 aggressive 的 scale augmentation, 因为小网络通常会欠拟合而不是过拟合 <strong>( 本人在 ENet 的实验中也印证了这点 )</strong>, 实验结论如下:</p><ol><li><em>pointwise group convolution</em> 有效, group 比不 group 好, Smaller models tend to benefit more from groups.</li><li>0.5x 情况下 group 比较大时会饱和, 准确率甚至下降. 但再减小 channel 到 0.25x, 发现增大 group 没有饱和, 反而收益更多了. <strong>( 此处实验不充分, 不足以证明什么 )</strong></li><li>Channel Shuffle 很有效, 特别在大 group 时.</li><li>实际加速由于内存访问等原因在移动平台上的呈现 理论加速 4 倍 = 实际加速 2.6 倍</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这部分是关于轻量级网络 ShuffleNet 的论文记录，主要是基于 channel shuffle 的想法来减少 CNN 中占大头的 1x1 卷积的计算量。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文笔记" scheme="https://lamply.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>DeepLab系列论文简略记录</title>
    <link href="https://lamply.github.io/2018/03/01/DeepLab/"/>
    <id>https://lamply.github.io/2018/03/01/DeepLab/</id>
    <published>2018-03-01T13:12:06.000Z</published>
    <updated>2019-10-12T07:12:36.999Z</updated>
    
    <content type="html"><![CDATA[<p>这部分是关于语义分割网络 DeepLab 系列的三篇论文。尽管经验性的技巧很多，但就效果而言还是很不错的，有不少值得参考的地方。<br><a id="more"></a><br><br></p><h3 id="DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs"><a href="#DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs" class="headerlink" title="DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"></a>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</h3><hr><h4 id="原文理解"><a href="#原文理解" class="headerlink" title="原文理解"></a>原文理解</h4><p>三个贡献:  </p><ol><li>明确表明了上采样滤波器或者叫 ‘空洞卷积’ 是 dense prediction 任务中的重要工具. 空洞卷积允许明确控制滤波器在计算特征响应后的分辨率, 也允许有效放大滤波器的感受野, 从而无计算量和参数量增加地聚合更大的上下文信息</li><li>提出了空间金字塔空洞池化 ( ASPP ), 能在多尺度上分割物体</li><li>结合 DCNN 和概率图模型提升边缘准确度</li></ol><h4 id="方法简述"><a href="#方法简述" class="headerlink" title="方法简述"></a>方法简述</h4><p>将后面一层 pooling 和 conv stride 改为 1, 换成 atrous conv, 最后并行多个不同 rate 的 atrous conv, 然后 fuse 在一起. 加上多尺度输入, COCO 预训练, randomly rescaling 扩增, CRF, 最终得出结果.</p><p><br><br><br></p><h3 id="Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation"><a href="#Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation" class="headerlink" title="Rethinking Atrous Convolution for Semantic Image Segmentation"></a>Rethinking Atrous Convolution for Semantic Image Segmentation</h3><hr><h4 id="原文理解-1"><a href="#原文理解-1" class="headerlink" title="原文理解"></a>原文理解</h4><p>针对多尺度分割, 设计了级联或并联的 atrous conv 模块, 改进了 ASPP, 没有 CRF 后处理也达到了 SOTA  </p><ul><li>Multi-grid Method<ul><li>一个 block 内几个卷积, 分不同的 dilation rate, 比如三个卷积则原来 { 1,1,1 } 可以变为 { 1,2,4 }, 然后乘上该 block 的 dilation rate. 也就是说本来要 stride 2 的, 改用 atrous conv 后, 后面 block 卷积的 dilation rate 为 2 x { 1,2,4 } = { 2,4,8 }</li><li>但是 ResNet 的 block 难道不是只有一个 3x3 ??? 1x1 哪来的 dilation ???? 原文没有提及, github 有相关讨论, 大致可能一是 google 所用 ResNet Block 加了三个 3x3 卷积, 二是指多个 bottleneck 内的 3x3 卷积做 multi-grid</li></ul></li><li>ASPP<ul><li>加上 batch normalization</li><li>dilation rate 过大会导致 valid 的 weights 减少 ( 非 pad 0 区域与 filters 区域相交减小 ), 这会导致大 dilation rate 的 filters 退化, 为解决这个问题, 且整合 global context, 使用了 image-level 的 feature. 特别的, 在模型最后的 feature map 采用 global average pooling, 然后送进 256 个 1x1 卷积 BN 中, 然后双线性插值到需要的维度.</li><li>最终 ASPP 有: 一个 1x1, 三个 3x3 dilation rate { 6,12,18 } ( 缩小 16 倍时 ), 以及 image-level feature, 最终 concat 在一起做 1x1 卷积. 其中卷积都是 256 output channel 和 BN.</li></ul></li></ul><p><strong><em>原文提及主要性能提升来自于 Batch Normalization 的引入及 COCO 预训练. 实际本人尝试在小数据集上从 16s 到 8s 冻结 Batch Normalization 做 finetune, 最终效果并没有提升, 可能真的要在大量数据下才能得到较好的 Batch Normalization 参数做初始化吧</em></strong>  </p><p><br><br><br></p><h3 id="Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation"><a href="#Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation" class="headerlink" title="Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"></a>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</h3><hr><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>融合了 Encoder-Decoder 和 SPP, 加上 depthwise 卷积的大量使用</p><h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><ol><li>Atrous Convolution 的 Encoder-Decoder  <ul><li>Encoder 阶段改进 Xception + DeepLabv3, 主要改进点为 <em>atrous separable convolution</em>, 其实就是都大量使用 depthwise 卷积替换 ASPP 等卷积结构, 可以显著的降低计算复杂度并保持相似或更好的性能  </li><li>Decoder 阶段在 DeepLabv3 输出端上采样 ( output_stride = 16 下 4 倍 ), 然后 concat 一个经过 1x1 降维后的网络低层特征, 再做 3x3 卷积, 然后上采样到原图大小. 这里也可以采用 depthwise 卷积提升效率.</li></ul></li><li>修改 Xception  <ul><li>除输入流外加入了更多的层数  </li><li>去掉 max-pooling, 以 stride depthwise 卷积替代  </li><li>所有 3x3 depthwise 后加 batchnorm 和 ReLU <strong><em>( 这个 ReLU 效果存疑 )</em></strong></li></ul></li></ol><h4 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h4><ol><li>关于 Decoder 的设计实验  <ul><li>这部分作者实验发现 Decoder 引入 before striding 的同分辨率 feature map 然后做 1x1 卷积压缩到 48 channel 再进行 concat 效果最好, 不过 mIoU 差距都比较小, 而且 64 channel 效果更差可以看出该选择可能与 Encoder 输出通道比例有关联, 玄学成分多些  </li><li>还有就是 concat 后两个 3x3 256 卷积性能最好, 而且只做一个 skip-connection 会更高效</li></ul></li><li>关于 Network Backbone<ul><li>这里比较重要的一点就是 Decoder 的加入会带来 1%~2% 的性能提升, 这点在训练和测试相同 output stride 的情况下会比较明显, 不同的情况下比较不明显. <strong><em>( 这里总体计算量会增加几十 B, 个人认为在轻量级网络下不划算, 还是希望能有不会明显增加计算量的 decoder ) </em></strong></li><li>另外, 作者使用 Xception 实验时发现 multi-grid 不会提升性能, 于是没有使用……………..</li><li>关于 pretrain, 在 COCO 上 pretrain 会带来大约 2% 的提升, 在 JFT 上 pretrain backbone 会再额外带来大约 1% 的提升</li></ul></li></ol><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>DeepLabv3+ 采用 Encoder-Decoder 结构, 将 DeepLabv3 作为 Encoder, 并引入简单高效的 skip-connection 来恢复边缘. 还有就是改进 Xception 以及采用 atrous separable convolution 降低计算量. 总的来说没什么新鲜的, 都是整合之前的方法然后通过大量实验找到的最优最高效的结构, 或许这就是炼金术吧……</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这部分是关于语义分割网络 DeepLab 系列的三篇论文。尽管经验性的技巧很多，但就效果而言还是很不错的，有不少值得参考的地方。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文笔记" scheme="https://lamply.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Coordinating Filters for Faster Deep Neural Networks</title>
    <link href="https://lamply.github.io/2017/11/15/Low_Rank/"/>
    <id>https://lamply.github.io/2017/11/15/Low_Rank/</id>
    <published>2017-11-15T08:45:11.000Z</published>
    <updated>2019-05-15T17:52:25.319Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="/2017/11/15/Low_Rank/ForceRegularization.png">  </div><p>这篇论文是在学习压缩模型时无意中看到的，发表在 ICCV 2017。因为看到它的 motivation 觉得挺有意思的（昴星团瞩目），刚好还有代码，于是就学习了一下，顺带看看能不能用在项目上。  </p><p>原文链接： <a href="https://arxiv.org/abs/1703.09746" target="_blank" rel="noopener">Coordinating Filters for Faster Deep Neural Networks</a><br><a id="more"></a></p><h3 id="原文理解"><a href="#原文理解" class="headerlink" title="原文理解"></a>原文理解</h3><ul><li>压缩和加速 DNN 模型的工作</li><li>常规压缩 <em>sparsity-based</em> 方法 <em>Low-Rank Approximations ( LRA )</em>:<ul><li>可以不必经过仔细的硬件/软件设计就能压缩和加速 DNN</li><li>原理在于滤波器之间冗余(相关性), 把大的矩阵近似成两个小矩阵相乘</li><li>此工作专注于压缩已经训练好的模型来达到最大化减小计算复杂性, 然后 retrain 来保持精度</li></ul></li><li>本工作注重训练出 <em>Lower-Rank Space</em> 的 DNN, 提出了 <em>Force Regularization</em>:<ul><li>主要是通过引入额外梯度 ( <em>attractive forces</em> ) 微调参数来增强滤波器的相关性, 从而使得 LRA 后能获得更小的参数量</li></ul></li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>首先介绍 cross-filter LRA :<br>LRA 为将大矩阵 </p><script type="math/tex; mode=display">W\in\mathbb{R}^{N \times C \times H \times W}</script><p>分解为一个低秩矩阵和一个1x1的卷积偏移</p><script type="math/tex; mode=display">\beta_m\in\mathbb{R}^{M \times C \times H \times W}, b\in\mathbb{R}^{1 \times C \times H \times W}</script><p>那么输出的 feature map 为:  </p><script type="math/tex; mode=display">O_n\approx(\sum^M_{m=1}b_m^{(n)}\beta_m)*I = \sum^M_{m=1}(b_m^{(n)}F_m)</script><p>这里的 </p><script type="math/tex; mode=display">F_m = \beta_m * I</script><p>所以输出即低秩矩阵与输入的卷积的线性组合  </p><p>然后是 <em>Force Regularization</em>:<br>从数学层面上看 <em>Force Regularization</em></p><script type="math/tex; mode=display">\Delta W_i = \sum^N_{j=1}\Delta W_{ij} = ||W_i||\sum^N_{j=1}(f_{ji}-f_{ji}w_i^Tw_i)</script><script type="math/tex; mode=display">W_i \gets W_i-\eta \cdot (\frac{\partial E(W)}{\partial W_i}-\lambda_s \cdot \Delta W_i)</script><p>这里E(W)为损失，λs 为 trade off 因子，\(f_{ji}\) 如下：  </p><div align="center"><img src="/2017/11/15/Low_Rank/fji.png">  <img src="/2017/11/15/Low_Rank/fig_math.png">  </div><p>在物理层面上看 <em>Force Regularization</em> , 像是引力将参数聚集在一起</p><blockquote><p>Suppose each vector \(w_i\) is a rigid stick and there is a particle fixed atthe endpoint. The particle has unit mass, and the stick is massless and can freely spin around the origin. Given the pair-wise attractive forces (e.g.,universal gravitation) f_ji, Eq. (2) is the acceleration of particle \(i\). As the forces are attractive, neighbor particles tend to spin around the origin to assemble together.  </p></blockquote><p>作者认为, 增加 <em>Force Regularization</em> 可以让一簇滤波器趋向于有相同的方向, 而由于数据损失梯度的存在使得该正则项不影响原本滤波器提取有判别力的特征的能力(存疑)</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>实验使用 baseline 作为 pretrained model. 原因是在相同最大迭代次数下, 从 baseline 开始训练比从头开始要有更好的精准度和速度提升的tradeoff, 因为 pretrained model 提供了精准度和高关联性的初始化条件.  </p><p>实验结论:</p><ul><li><em>Force Regularization</em> 能在低层卷积保持低秩特性, 然后在高层卷积时有很大的压缩, 总体上看 rank ratio (低秩和全秩比) 大约为50%.  </li><li>L2norm 在高 rank ratio 时表现得比较好, L1norm 在潜在低 rank ratio 时表现得更好.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/2017/11/15/Low_Rank/ForceRegularization.png&quot;&gt;  
&lt;/div&gt;

&lt;p&gt;这篇论文是在学习压缩模型时无意中看到的，发表在 ICCV 2017。因为看到它的 motivation 觉得挺有意思的（昴星团瞩目），刚好还有代码，于是就学习了一下，顺带看看能不能用在项目上。  &lt;/p&gt;
&lt;p&gt;原文链接： &lt;a href=&quot;https://arxiv.org/abs/1703.09746&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Coordinating Filters for Faster Deep Neural Networks&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文笔记" scheme="https://lamply.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle &quot;Amazon from Space&quot; 经验分享</title>
    <link href="https://lamply.github.io/2017/10/17/Kaggle/"/>
    <id>https://lamply.github.io/2017/10/17/Kaggle/</id>
    <published>2017-10-17T05:13:06.000Z</published>
    <updated>2019-07-30T01:54:53.159Z</updated>
    
    <content type="html"><![CDATA[<p>看了 Kaggle 亚马逊雨林卫星图分类比赛第一名 <a href="http://blog.kaggle.com/2017/10/17/planet-understanding-the-amazon-from-space-1st-place-winners-interview/" target="_blank" rel="noopener">Planet: Understanding the Amazon from Space, 1st Place Winner’s Interview</a>, 学到了些 trick, 这里记录一下<br><a id="more"></a></p><ol><li><p>关于 precision 和 recall 的权衡, 可以在原有的 log loss 上增加 F_beta - score loss 来达到, 如:</p><script type="math/tex; mode=display">F_1 = 2\cdot\frac{precision\cdot recall}{precision+recall}</script><p>而</p><script type="math/tex; mode=display">F_\beta = (1+\beta^2)\cdot\frac{precision\cdot recall}{(\beta^2\cdot precision)+recall}</script><p>使用 F2-score 时, 代表 recall 比 precision 重要（由于 β 因子大于 1，recall 的增幅对 score 要更明显）, F0.5-score, 代表 precision 比 recall 重要（同理）  </p></li><li><p>关于分类标签变量存在相关或部分对立的情况下的预测, 可以对最后一层输出的概率使用 ridge regression ( 岭回归, 即加了 L2 正则项的最小二乘, 鼓励回归参数尽可能利用到所有的相关变量, 惩罚可能存在的个别变量 dominant 的大正负值参数 cancel 得出结果的情况 ). 该方法也可以用在 ensemble 多个模型上, 最终对输出结果做 ridge regression</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;看了 Kaggle 亚马逊雨林卫星图分类比赛第一名 &lt;a href=&quot;http://blog.kaggle.com/2017/10/17/planet-understanding-the-amazon-from-space-1st-place-winners-interview/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Planet: Understanding the Amazon from Space, 1st Place Winner’s Interview&lt;/a&gt;, 学到了些 trick, 这里记录一下&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术经验" scheme="https://lamply.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BB%8F%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>ResNet</title>
    <link href="https://lamply.github.io/2017/09/01/ResNet/"/>
    <id>https://lamply.github.io/2017/09/01/ResNet/</id>
    <published>2017-09-01T09:00:06.000Z</published>
    <updated>2019-10-12T09:44:08.595Z</updated>
    
    <content type="html"><![CDATA[<p>这里是 ResNet 的论文，作为被广泛应用的骨干网络，它提出的几个概念可以说是<strong>着实可靠</strong>地拓宽了网络设计的思路，对于广大摸着石头过河的工程师和研究者来说就是指出了一条明路。网络本身也高效、简洁且实用，可以作为 VGG 的上位替代。<br><a id="more"></a></p><h2 id="Deep-Residual-Learning-for-Image-Recognition"><a href="#Deep-Residual-Learning-for-Image-Recognition" class="headerlink" title="Deep Residual Learning for Image Recognition"></a>Deep Residual Learning for Image Recognition</h2><hr><h4 id="原文理解"><a href="#原文理解" class="headerlink" title="原文理解"></a>原文理解</h4><h5 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h5><p>网络深度对很多视觉任务都有很强的重要性, 但是仅靠简单的层堆积会存在梯度消失/爆炸问题. 虽然使用一些归一化初始技巧和中间归一化层可以很大程度解决这些问题, 但随着深度继续增加, 网络的精准度会逐渐饱和, 接着就会快速劣化, 这就是 <em>degradation problem</em>. 出乎意料的是, 这些问题不是因为过拟合导致的, 实验证明增加更多的层数会导致更高的 <em>训练误差</em>.<br>本文, 作者通过引入一个深度残差学习框架解决了这个 degradation problem. 与其让每几个堆叠的层直接学习潜在的映射, 我们显式地让这些层去学习残差映射. 正式来讲, 即输出 H(x), 输入 x, 让非线性层学习 F(x):=H(x)-x, 那么原本的映射就变成了 F(x)+x. 这在前向网络中会被视为捷径 ( “<em>shortcut connections</em>“ ), 允许网络跳级连接, 在本文中可以简单看成 <em>恒等映射 ( “identity<br>mapping” )</em>.<br>经过实验发现, 深度残差网很容易训练, 且很容易让精度随网络深度的增加而增加.  </p><h5 id="残差学习"><a href="#残差学习" class="headerlink" title="残差学习"></a>残差学习</h5><p>假设多次非线性函数能够渐进逼近复杂函数, 那么在维度不变的情况下该假设与渐进逼近残差是一样的.<br>之所以改为学习残差, 是因为 degradation problem 这个反常的现象. 在残差学习的恒等映射情况下, 一个更深的模型的训练误差是应该不超过比它浅的模型的. degradation problem 告诉我们 solver 使用多次非线性函数在渐进逼近恒等映射时可能存在困难, 而残差学习则提供了将权重趋向 0 来达到恒等映射的拟合.<br>当然, 在现实中, 恒等映射不太可能是最优的. 但这个改动可能有助于先决这个问题. 如果最优函数接近于恒等映射而非零映射, 那么 solver 应该很容易学习扰动来得到最优函数. 通过实验发现, 通常残差函数只有很小的响应, 这表明恒等映射的可以提供合理的先决条件.</p><h5 id="shortcut-恒等映射"><a href="#shortcut-恒等映射" class="headerlink" title="shortcut 恒等映射"></a>shortcut 恒等映射</h5><p>本文采用每叠一些层就使用残差学习, 也就是建立一个 block:</p><script type="math/tex; mode=display">y=F(x,W)+x</script><p>其中:</p><script type="math/tex; mode=display">F = W_2\sigma(W_1x)</script><p>σ 为 ReLU, 省略了 bias. 在得到 y 后再做一次 ReLU.<br>如果需要变化维度, 则:</p><script type="math/tex; mode=display">y=F(x,W)+W_sx</script><p>W_s 为投影矩阵, 用来改变维度, 当然它也可以是个方阵用作线性变换, 但后续实验证明恒等变换就足够解决 degradation problem 了.<br>F 是可以灵活改变的, 可以含有多层, 或者不同类型的层, 比如卷积层. 但如果 F 仅含有一层映射的话, 那还没有得到有效的效果验证.</p><h5 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h5><p>为了对比, 作者构建了两个相同计算量的网络, 两种网络均为类似 VGG 的网络. 开始 7x7 stride 2, 然后 3x3, 每降低一次 feature map, double 一次 filters 数量, 使用 stride 2 卷积来降低 feature map, 堆叠 34 层和类似的 18 层. 最后做全局平均池化, 然后接 1000 分类器, 卷积后面都采用了 batch normalization 保证训练不失败.<br>不同的是 ResNet 版加入了跳级连接, 降低 feature map 提升维度处分成两种策略, 一种填充 0 来扩增维度, 另一种上述提到的 1x1 升维, 两种方法都 stride 2 降低空间大小.<br>实验得知, VGG 34 层表现比 18 层要差. 而且经过更多 iter 的训练发现问题没有解决.<br>对于 ResNet 版, 采取第一种策略的, 34 层比 18 层错误率要少 2.8%, 更重要的, 34 层表现出明显更低的训练误差, 并且能很好泛化验证数据.<br>对比两种网络的 18 层版, 发现 ResNet 能在训练早期更快的收敛.  </p><p>ImageNet 上 10-crop top-1 error 实验结果如下:</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">plain</th><th style="text-align:center">ResNet</th></tr></thead><tbody><tr><td style="text-align:center">18 layers</td><td style="text-align:center">27.94</td><td style="text-align:center">27.88</td></tr><tr><td style="text-align:center">34 layers</td><td style="text-align:center">28.54</td><td style="text-align:center">25.03</td></tr></tbody></table></div><p>(PS: <em>看上去似乎浅层下 ResNet 影响不大</em>)</p><h5 id="恒等和投影-shortcuts"><a href="#恒等和投影-shortcuts" class="headerlink" title="恒等和投影 shortcuts"></a>恒等和投影 shortcuts</h5><p>对比 ResNet 不同策略, 分为三种对比: </p><ol><li>零填充升维, 所有 shortcuts 都没有参数</li><li>升维时使用 1x1 卷积升维, 其他恒等</li><li>所有 shortcuts 都带有 1x1 卷积<br>实验表明, 2 稍微比 1 好些 <code>(top-1 25.03-&gt;24.52)</code>, 作者认为这是因为 1 在升维时的零填充部分实际并没有进行残差学习; 3 轻微比 2 好些 <code>(top-1 24.52-&gt;24.19)</code>, 作者认为这是因为 3 在 shortcuts 处引入了额外参数.  微小的差异表明投影 shortcuts 对与解决 degradation problem 不重要. </li></ol><h5 id="更深的瓶颈结构"><a href="#更深的瓶颈结构" class="headerlink" title="更深的瓶颈结构"></a>更深的瓶颈结构</h5><p>为了节省训练时间, 作者把上面的 block 修改为 瓶颈 ( <em>bottleneck</em> )  设计. 即在每个 $F$ 内堆叠三层, 分别为 1x1, 3x3, 1x1 卷积层. 其中 1x1 用于升降维度, 使 3x3 作为有更低输入输出维度的瓶颈.  </p><p><div align="center"><img src="/2017/09/01/ResNet/block.png">  </div><br>对于瓶颈结构来说, shortcuts 策略的选择对模型大小和时间复杂度都有很重要的影响, 无参数恒等 shortcuts 可以让瓶颈模型更高效. (<em><code>?</code></em>)<br>作者在 50/101/152 层模型中使用了瓶颈结构和策略2的升降维.其各个配置如下:   </p><p><div align="center"><img src="/2017/09/01/ResNet/Configuration.png">  </div><br>(PS: <em>值得注意的是 ResNet 50 层瓶颈版的 FLOPs 与 34 层相差不大, 网络的堆叠配置是一样的</em>)</p><p>在 ImageNet 验证集上的 10-crop 实验结果如下:</p><div class="table-container"><table><thead><tr><th style="text-align:center">method</th><th style="text-align:center">top-1 err.</th><th style="text-align:center">top-5 err.</th></tr></thead><tbody><tr><td style="text-align:center">plain-34</td><td style="text-align:center">28.54</td><td style="text-align:center">10.02</td></tr><tr><td style="text-align:center">ResNet-34 1</td><td style="text-align:center">25.03</td><td style="text-align:center">7.76</td></tr><tr><td style="text-align:center">ResNet-34 2</td><td style="text-align:center">24.52</td><td style="text-align:center">7.46</td></tr><tr><td style="text-align:center">ResNet-34 3</td><td style="text-align:center">24.19</td><td style="text-align:center">7.40</td></tr><tr><td style="text-align:center">ResNet-50</td><td style="text-align:center">22.85</td><td style="text-align:center">6.71</td></tr><tr><td style="text-align:center">ResNet-101</td><td style="text-align:center">21.75</td><td style="text-align:center">6.05</td></tr><tr><td style="text-align:center">ResNet-152</td><td style="text-align:center">21.43</td><td style="text-align:center">5.71</td></tr></tbody></table></div><p>在 ImageNet 验证集上单模型实验结果如下: </p><div class="table-container"><table><thead><tr><th style="text-align:center">method</th><th style="text-align:center">top-1 err.</th><th style="text-align:center">top-5 err.</th></tr></thead><tbody><tr><td style="text-align:center">ResNet-34 2</td><td style="text-align:center">21.84</td><td style="text-align:center">5.71</td></tr><tr><td style="text-align:center">ResNet-34 3</td><td style="text-align:center">21.53</td><td style="text-align:center">5.60</td></tr><tr><td style="text-align:center">ResNet-50</td><td style="text-align:center">20.74</td><td style="text-align:center">5.25</td></tr><tr><td style="text-align:center">ResNet-101</td><td style="text-align:center">19.87</td><td style="text-align:center">4.60</td></tr><tr><td style="text-align:center">ResNet-152</td><td style="text-align:center">19.38</td><td style="text-align:center">4.49</td></tr></tbody></table></div><p>最终在 ILSVRC 2015 提交中使用了两个 152 层模型的 ensembles, 得到 top-5 error 3.57.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这里是 ResNet 的论文，作为被广泛应用的骨干网络，它提出的几个概念可以说是&lt;strong&gt;着实可靠&lt;/strong&gt;地拓宽了网络设计的思路，对于广大摸着石头过河的工程师和研究者来说就是指出了一条明路。网络本身也高效、简洁且实用，可以作为 VGG 的上位替代。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文简译" scheme="https://lamply.github.io/tags/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>GoogLeNet系列</title>
    <link href="https://lamply.github.io/2017/09/01/GoogLeNet/"/>
    <id>https://lamply.github.io/2017/09/01/GoogLeNet/</id>
    <published>2017-09-01T09:00:06.000Z</published>
    <updated>2019-10-12T09:24:08.075Z</updated>
    
    <content type="html"><![CDATA[<p>这部分是关于 GoogLeNet 系列网络的两篇论文，涵盖了 Inception v1 到 v3。作为 CNN 发展进程中经典的模型，它通过大量实验思考总结了很多关于 CNN 在设计方面应当注意的事项，尽管没有 VGG 那般简洁好用易训练，而且工程设计感很浓重，但其中涉及到的各种实验和结果都是实打实的，对这些实验的解读可以印证和加深自己对深度神经网络的理解，建议参考原文。  </p><p>Going deeper with convolutions:   <a href="https://arxiv.org/pdf/1409.4842.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1409.4842.pdf</a>  </p><p>Rethinking the Inception Architecture for Computer Vision:<br><a href="https://arxiv.org/pdf/1512.00567.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1512.00567.pdf</a>  </p><a id="more"></a><p><br></p><h2 id="Going-deeper-with-convolutions"><a href="#Going-deeper-with-convolutions" class="headerlink" title="Going deeper with convolutions"></a>Going deeper with convolutions</h2><hr><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><ul><li>介绍一种新的结构, <strong>Inception</strong>, 想法基于Hebbian principle和multi-scale processing的直觉, 可以在保持计算量恒定下加深和加宽网络</li><li>ILSVRC 2014: 相比AlexNet, GoogLeNet取得小12倍的参数以及高准确率</li></ul><h4 id="动机和思考"><a href="#动机和思考" class="headerlink" title="动机和思考"></a>动机和思考</h4><ul><li>性能瓶颈, 增强性能要增加模型大小, 但又加大了过拟合可能以及对数据的需求和计算量</li><li>解决之根在于稀疏化结构, 联合 Hebbian principle (<code>neurons that fire together, wire together</code>) : <blockquote><p>Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs.  ( : <em>“Provable bounds for learning some deep representations”</em> )</p></blockquote></li><li><p>问题在于当前计算设备对非固定形状的稀疏矩阵的数值计算不够高效. 使用 lookups 和 cache 带来的巨大开销使得稀疏计算得不偿失. 那么是否能够利用计算设备对 dense matrices 高效计算的优势, 在稀疏的架构上进行 dense matrices 的计算? 大量文献表明: </p><blockquote><p>clustering sparse matrices into relatively dense submatrices tends to give state of the art practical performance for sparse matrix multiplication. ( : <em>“On two-dimensional sparse matrix partitioning: Models, methods, and a recipe.”</em>  )</p></blockquote><p>相信不久的未来类似的方法会应用到自动构建的非固定形状的深度学习架构上</p></li><li>还有一个问题, 尽管推出的结构在计算机视觉上取得了成功, 但仍然有疑惑这种成功是否能归因于指导这种网络构建的思想. 这个问题即是在于自动系统是否能使用相似的算法在其他领域创建一个总体架构看上去相差很远, 但有相似的增益效果的网络拓扑</li></ul><h4 id="结构细节"><a href="#结构细节" class="headerlink" title="结构细节"></a>结构细节</h4><ul><li>Inception 核心思想基于如何在卷积视觉网络中找到最优局部稀疏结构, 这种结构能被 dense components 近似和覆盖</li><li>上层多输入聚合到下层单输出, 能够被1x1卷积表示, 加上大卷积聚合多尺度区域, 为了方便, Inception 取 1x1 3x3 和 5x5, 加入额外的并行pooling 通道也会对效果有所帮助.</li><li><p>高层后空间聚合需要减少, 于是要提高 3x3 5x5 卷积的比例</p><div align="center"><img src="/2017/09/01/GoogLeNet/Inception-Naive.png">  </div></li><li><p>为了不让后期 5x5 与 pooling聚合(限定输入输出同 channel ) 导致的计算量高昂, 加入 1x1 卷积降维 ( <code>even low dimensional embeddings might contain a lot of information about a relatively large image patch</code> )</p><div align="center"><img src="/2017/09/01/GoogLeNet/Inception-Reduction.png">  </div></li></ul><h4 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h4><blockquote><p>Still, one prescription that was verified to work very well after the competition includes sampling of various sized patches of the image whose size is distributed evenly between 8% and 100% of the image area and whose aspect ratio is chosen randomly between 3/4 and 4/3. Also, we found that the photometric distortions by Andrew Howard [8] were useful to combat overfitting to some extent.</p></blockquote><h4 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h4><p>该结构在detection中也很有竞争力, 尽管没有做其他优化, 说明 Inception 结构的有效性, 会启发以后的模型结构趋向于 sparse. </p><p><br><br><br></p><h3 id="Rethinking-the-Inception-Architecture-for-Computer-Vision"><a href="#Rethinking-the-Inception-Architecture-for-Computer-Vision" class="headerlink" title="Rethinking the Inception Architecture for Computer Vision"></a>Rethinking the Inception Architecture for Computer Vision</h3><hr><h4 id="原文理解"><a href="#原文理解" class="headerlink" title="原文理解"></a>原文理解</h4><h5 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍:"></a>介绍:</h5><ul><li>上一篇文章提出的 Inception 略复杂很难去修改网络, 如果只是简单地加大网络会导致计算性能优势瞬间消失. 而且上一篇文章并没有详细表明设计时各部分的影响因素, 所以很难让它用于新的领域同时保持高效率. 比如, 如果要加 Inception-style 的网络的容量, 最简单的只能加倍滤波器数量, 但这样会导致计算量和参数的4倍增加. 这很不友好, 特别如果效果甚微的话.</li><li>所以这篇文章开始讲述一些通用的原理和优化 ideas, 这些都是已经验证过对高效加大网络有用的方法.</li><li>广义上的 Inception-style 结构的搭建对这些约束是可以比较灵活自然的. 这是因为它大量使用了降维和并行结构, 这让它减轻了临近结构改变带来的影响. 当然, 为了保持模型的高质量, 使用 Inception 还是需要遵循一些指导原则.</li></ul><h5 id="通用设计原理"><a href="#通用设计原理" class="headerlink" title="通用设计原理:"></a>通用设计原理:</h5><p>这里会描述一些通过对不同结构的 CNN 的大规模实验验证过的设计原理. 尽管有点投机的意味, 但严重背离这些原则趋向于恶化网络的质量, 改了之后一般都会有所提升. </p><ol><li>避免表达瓶颈 ( representational bottlenecks ), 特别是在网络的早期. 前向网络能表达为一个信息由输入到分类或回归器的有向无环图 ( acyclic graph ), 所以信息流方向是确定的, 输入到输出之间存在大量信息, 应该避免因极度压缩导致的瓶颈. 一般 表征 ( representation ) 的大小应该由输入到输出平缓地降低直到得到要直接用于任务的大小. 理论上, 信息内容不能仅仅以表征的维度来评判, 因为它舍弃了比如关联结构 ( correlation structure ) 等重要因素；维度仅仅用于提供信息内容的粗略估计. </li><li>高维表征更容易在网络局部中被处理. 增加每个卷积网络的 tile 的 activations 允许更多的解开的 ( disentangled ) 特征, 能加快网络训练.</li><li>空间聚合能通过较低维嵌入完成, 而且还不会损失表达力. 比如, 在进行一个更分散 ( more spread out ) 的卷积 ( 比如3x3 ) 之前, 在空间聚合之前降低输入表征的维度不会导致严重的不良影响. 我们推测临近的单元存在很强的关联, 如果输出是用于空间聚合的话, 导致降维损失的信息会很少. 鉴于这些信号应该很容易压缩，维度的减小甚至加快了学习.</li><li>平衡网络的宽度. 最优化网络的性能可以通过平衡每阶段的滤波器的数量和网络的深度来完成. 增加宽度和深度会使网络更高质量, 但要恒定计算量最优化提升需要两者并行增加. 计算预算因此应该用于平衡网络深度和宽度.  </li></ol><p>尽管好像挺有道理, 但以上的原则并不能直接拿来即用, 仅仅用在模棱两可的情况下会比较明智.</p><h5 id="分解卷积"><a href="#分解卷积" class="headerlink" title="分解卷积:"></a>分解卷积:</h5><ol><li><strong>分解成小卷积: </strong>通过使用临近结果将 5x5 分解成两个 3x3, 并在分解卷积间使用非线性激活.</li><li><strong>非对称卷积: </strong>将 nxn 卷积分解成 1xn + nx1 卷积可以进一步分解卷积, 但该方法在网络刚开始的地方不那么 work well, 但在中等大小的 feature map ( mxm 大小,  m 在 12 到 20 之间 ) 上能取得很好的效果, 这种时候使用 1x7 和 7x1 卷积能达到很好的效果.</li></ol><h5 id="辅助分类器的效用"><a href="#辅助分类器的效用" class="headerlink" title="辅助分类器的效用"></a>辅助分类器的效用</h5><p>发现 辅助分类器存在与否在模型达到高准确率之前没有影响, 两者训练过程几乎完全相同. 只有在接近训练的尾声时, 带有辅助分类器的模型才会超过没有辅助分类器的模型, 最终达到一个稍高一点的收敛结果.<br>此外, 此前的 GoogLeNet 使用了两个辅助分类器, 而接近输入端的辅助分类器实际上去掉也不会有负面影响. 所以之前关于它们对低层特征的帮助的假设更可能时错误的. 不过作者认为辅助分类器发挥了正则化的作用, 因为主分类器的性能会由于旁路实施了 batch-normalized 或者 dropout. </p><h5 id="高效的网格缩小"><a href="#高效的网格缩小" class="headerlink" title="高效的网格缩小"></a>高效的网格缩小</h5><p>传统的卷积网络用 pooling 来减少特征图网格大小, 为了避免表达瓶颈, 可以使用先拓宽维度再 pooling 的方法. 比如, 输入 d x d 大小 k 通道, 输出  d/2 x d/2 大小 2k 通道, 那就先做 2k stride 1 的卷积, 然后再接上 pooling. 但这样计算消耗太大, 于是作者提出分成两支, 使用 stride 2 并行 pooling 和 卷积, 最后 concat 在一起.</p><h5 id="Inception-v2"><a href="#Inception-v2" class="headerlink" title="Inception-v2"></a>Inception-v2</h5><p>基于上述内容作者提出了改进版的 Inception, 网络大致变化是:</p><ol><li>开局 7x7 变成 3 个 3x3. </li><li>Inception 分成三类: <ul><li>第一类用于 35x35 的 feature map, 即分解 5x5 后的传统 Inception</li><li>第二类用于 17x17 的 feature map, 即非对称卷积版 Inception, 卷积核采用 7x7 大小</li><li>第三类用于 8x8 高维特征的 feature map, 即使用并行非对称卷积加大宽度后的 Inception</li></ul></li><li>每类 Inception 间采用上述的高效的网格缩小方法</li></ol><h5 id="使用-Label-Smoothing-正则化模型"><a href="#使用-Label-Smoothing-正则化模型" class="headerlink" title="使用 Label Smoothing 正则化模型"></a>使用 Label Smoothing 正则化模型</h5><p>一般 label 是离散且 one-hot 的, 而使用最小化 cross entropy 来训练的模型的话相当于对 label 做最大拟然估计, 其数学形式是 Dirac delta :   </p><script type="math/tex; mode=display">q(k)=\delta_{k,y}</script><p>在有限参数的情况下这是无法实现完全拟合的, 但模型却会趋向于这种形式, 这会导致两个问题: 一是不能保证泛化性能导致过拟合, 二是这种方式鼓励加大正确的预测和其他不正确预测之间的间隔, 加上在冲激处很大的梯度, 会导致模型的适应力下降. 直观来说就是对其预测太过自信了.<br>为了降低模型的自信, 可以采用一个很简单的方法, 即将 label 的分布函数  </p><script type="math/tex; mode=display">q(k|x)=\delta_{k,y}</script><p>改成  </p><script type="math/tex; mode=display">q'(k|x)=(1-\epsilon)\delta_{k,y}+\epsilon u(k)</script><p>就是混上了一个固定的分布函数 u(k), 再经过平滑因子 ϵ 权重. 作者此处使用了平均分布 u(k)=1/K, 所以</p><script type="math/tex; mode=display">q'(k|x)=(1-\epsilon)\delta_{k,y}+\frac{\epsilon}{K}</script><p>这样每个 k 都有一个最低值, 而最高值的影响被平滑影响. 在损失函数的角度上看也可以认为 u(k) 提供了正则化项.<br>在 ILSVRC 2012 中, 作者采用了 u(k) = 1/1000 以及 ϵ = 0.1. 最终得到了恒定的 0.2% 的 top-1 和 top-5 效果提升.</p><h5 id="低分辨率输入的性能"><a href="#低分辨率输入的性能" class="headerlink" title="低分辨率输入的性能"></a>低分辨率输入的性能</h5><p>为了比较不同输入分辨率对精准度的影响, 作者做个实验. 对三种输入分出三种网络配置 ( 为了公平对比保持计算量恒定) : </p><ol><li>299 × 299 receptive field with <strong>stride 2</strong> and maximum pooling after the first layer.</li><li>151 × 151 receptive field with <strong>stride 1</strong> and maximum pooling after the first layer.</li><li>79 × 79 receptive field with <strong>stride 1</strong> and <strong>without</strong> pooling after the first layer.</li></ol><p>结果如下:</p><div class="table-container"><table><thead><tr><th style="text-align:center">Receptive Field Size</th><th style="text-align:center">Top-1 Accuracy (single frame)</th></tr></thead><tbody><tr><td style="text-align:center">79 × 79</td><td style="text-align:center">75.2%</td></tr><tr><td style="text-align:center">151 × 151</td><td style="text-align:center">76.4%</td></tr><tr><td style="text-align:center">299 × 299</td><td style="text-align:center">76.6%</td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这部分是关于 GoogLeNet 系列网络的两篇论文，涵盖了 Inception v1 到 v3。作为 CNN 发展进程中经典的模型，它通过大量实验思考总结了很多关于 CNN 在设计方面应当注意的事项，尽管没有 VGG 那般简洁好用易训练，而且工程设计感很浓重，但其中涉及到的各种实验和结果都是实打实的，对这些实验的解读可以印证和加深自己对深度神经网络的理解，建议参考原文。  &lt;/p&gt;
&lt;p&gt;Going deeper with convolutions:   &lt;a href=&quot;https://arxiv.org/pdf/1409.4842.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1409.4842.pdf&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Rethinking the Inception Architecture for Computer Vision:&lt;br&gt;&lt;a href=&quot;https://arxiv.org/pdf/1512.00567.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1512.00567.pdf&lt;/a&gt;  &lt;/p&gt;
    
    </summary>
    
    
      <category term="论文简译" scheme="https://lamply.github.io/tags/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%91/"/>
    
  </entry>
  
</feed>
